{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Opteryx is a SQL query engine to query large data sets designed to run in low-cost serverless environments. Use Cases Referencing data from mulitple data platforms into a single query. Great for use in cost-optimized environments, where a traditional data solution like Hadoop is out of reach. Querying ad hoc data stores, where data is collected by logging systems and output to storage. Where you have many different environments, and each would require their own database to query static data. Where you occassionally query data, and don't want the effort of loading into a database or the cost of maintaining a database for infrequent use.","title":"Overview"},{"location":"#overview","text":"Opteryx is a SQL query engine to query large data sets designed to run in low-cost serverless environments.","title":"Overview"},{"location":"#use-cases","text":"Referencing data from mulitple data platforms into a single query. Great for use in cost-optimized environments, where a traditional data solution like Hadoop is out of reach. Querying ad hoc data stores, where data is collected by logging systems and output to storage. Where you have many different environments, and each would require their own database to query static data. Where you occassionally query data, and don't want the effort of loading into a database or the cost of maintaining a database for infrequent use.","title":"Use Cases"},{"location":"Getting%20Started/","text":"Getting Started Installation Install from PyPI (recommended) This will install the latest release version. pip install --upgrade opteryx Install from GitHub The lastest version, including pre-release and beta versions can be installed, this is not recommended for production environments. pip install git+https://github.com/mabel-dev/opteryx","title":"Getting Started"},{"location":"Getting%20Started/#getting-started","text":"","title":"Getting Started"},{"location":"Getting%20Started/#installation","text":"Install from PyPI (recommended) This will install the latest release version. pip install --upgrade opteryx Install from GitHub The lastest version, including pre-release and beta versions can be installed, this is not recommended for production environments. pip install git+https://github.com/mabel-dev/opteryx","title":"Installation"},{"location":"Client%20Connectivity/01%20Python/","text":"Python Embedded Python DBAPI Opteryx implements a partial Python DBAPI (PEP-0249) interface. import opteryx conn = opteryx . connect () cur = conn . cursor () cur . execute ( 'SELECT * FROM $planets' ) rows = cur . fetchall ()","title":"Python Embedded"},{"location":"Client%20Connectivity/01%20Python/#python-embedded","text":"","title":"Python Embedded"},{"location":"Client%20Connectivity/01%20Python/#python-dbapi","text":"Opteryx implements a partial Python DBAPI (PEP-0249) interface. import opteryx conn = opteryx . connect () cur = conn . cursor () cur . execute ( 'SELECT * FROM $planets' ) rows = cur . fetchall ()","title":"Python DBAPI"},{"location":"Contributing%20Guide/01%20Community/","text":"Community GitHub We use GitHub to host the code, perform testing, report bugs and maintain documentation. Slack Join us on Slack Etiquettes to follow 1. Be nice to everyone 2. Check off your resolved questions If you have received a useful reply to your question, please drop a \u2705 reaction or a reply for affirmation. 3. Try not to repost question If you have asked a question and have not got a response in 24hrs, please review your question for clarity and revise it. 4. Post in public Please don't direct message any individual member of Mabel community without their explicit permission, independent of reason. Your question might be helpful for other community members. 5. Don't spam tags Mabel and Opteryx are supported by volunteers, avoid tagging members unless it is urgent. 6. Use threads for discussion To keep the main channel area clear, we request to use threads to keep an ongoing conversation organized.","title":"Community"},{"location":"Contributing%20Guide/01%20Community/#community","text":"","title":"Community"},{"location":"Contributing%20Guide/01%20Community/#github","text":"We use GitHub to host the code, perform testing, report bugs and maintain documentation.","title":"GitHub"},{"location":"Contributing%20Guide/01%20Community/#slack","text":"Join us on Slack","title":"Slack"},{"location":"Contributing%20Guide/01%20Community/#etiquettes-to-follow","text":"1. Be nice to everyone 2. Check off your resolved questions If you have received a useful reply to your question, please drop a \u2705 reaction or a reply for affirmation. 3. Try not to repost question If you have asked a question and have not got a response in 24hrs, please review your question for clarity and revise it. 4. Post in public Please don't direct message any individual member of Mabel community without their explicit permission, independent of reason. Your question might be helpful for other community members. 5. Don't spam tags Mabel and Opteryx are supported by volunteers, avoid tagging members unless it is urgent. 6. Use threads for discussion To keep the main channel area clear, we request to use threads to keep an ongoing conversation organized.","title":"Etiquettes to follow"},{"location":"Contributing%20Guide/CODE_OF_CONDUCT/","text":"Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement via Slack (@joocer) or GitHub (@joocer). All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of, or an extreme violation, of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.","title":"Code of Conduct"},{"location":"Contributing%20Guide/CODE_OF_CONDUCT/#code-of-conduct","text":"","title":"Code of Conduct"},{"location":"Contributing%20Guide/CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"Contributing%20Guide/CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"Contributing%20Guide/CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"Contributing%20Guide/CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"Contributing%20Guide/CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement via Slack (@joocer) or GitHub (@joocer). All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"Contributing%20Guide/CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"Contributing%20Guide/CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"Contributing%20Guide/CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"Contributing%20Guide/CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"Contributing%20Guide/CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of, or an extreme violation, of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"Contributing%20Guide/CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.","title":"Attribution"},{"location":"Contributing%20Guide/CONTRIBUTING/","text":"Contribution Guide We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow. We Develop with GitHub All submissions, including submissions by core project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests. All Change must meet Quality Bars We have included a number of tests which run automatically when code is submitted using GitHub Actions. These tests must pass before a change is elligible to be merged. These tests include a regression test suite, security tests and other quality checks. All contributions will be under Apache 2.0 Licence The project is licensed under Apache 2.0 , your submissions will be under this same licence. Format and Style Guidance Code Style For consistent style, code should look like: Imports on separate lines ( imports then froms ) Variables should be in snake_case Classes should be in PascalCase Constants should be in UPPER_CASE Methods have docstrings Black formatted Self-explanatory method, class and variable names Type hints, especially in function definitions Code should have: Corresponding unit/regression tests Attributed external sources - even if there is no explicit license requirement A note about comments: Computers will interpret anything, humans need help interpreting code Spend time writing readable code rather than verbose comments Humans struggle with threading, recursion, parallelization, variables called x and more than 10... of anything Comments should usually be more than just the code in other words Good variable names and well-written code doesn't always need comments Code pushes should have prefixes: FIX\\#nn merges which fix GitHub issues (#nn represents the GitHub issue number) FEATURE\\#nn merges which implement GitHub feature requests (#nn represents the GitHub issue number) [DOCS] improvements to documentation Docstrings should look like this for non-trivial methods: def sample_method ( param_1 : str , param_2 : Optional [ int ] = None ) -> bool : \"\"\" A short description of what the method does. Parameters: param_1: string describe this parameter param_2: integer (optional) describe this parameter, if it's optional, what is the default Returns: boolean \"\"\"","title":"Contribution Guide"},{"location":"Contributing%20Guide/CONTRIBUTING/#contribution-guide","text":"We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow.","title":"Contribution Guide"},{"location":"Contributing%20Guide/CONTRIBUTING/#we-develop-with-github","text":"All submissions, including submissions by core project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.","title":"We Develop with GitHub"},{"location":"Contributing%20Guide/CONTRIBUTING/#all-change-must-meet-quality-bars","text":"We have included a number of tests which run automatically when code is submitted using GitHub Actions. These tests must pass before a change is elligible to be merged. These tests include a regression test suite, security tests and other quality checks.","title":"All Change must meet Quality Bars"},{"location":"Contributing%20Guide/CONTRIBUTING/#all-contributions-will-be-under-apache-20-licence","text":"The project is licensed under Apache 2.0 , your submissions will be under this same licence.","title":"All contributions will be under Apache 2.0 Licence"},{"location":"Contributing%20Guide/CONTRIBUTING/#format-and-style-guidance","text":"","title":"Format and Style Guidance"},{"location":"Contributing%20Guide/CONTRIBUTING/#code-style","text":"For consistent style, code should look like: Imports on separate lines ( imports then froms ) Variables should be in snake_case Classes should be in PascalCase Constants should be in UPPER_CASE Methods have docstrings Black formatted Self-explanatory method, class and variable names Type hints, especially in function definitions Code should have: Corresponding unit/regression tests Attributed external sources - even if there is no explicit license requirement A note about comments: Computers will interpret anything, humans need help interpreting code Spend time writing readable code rather than verbose comments Humans struggle with threading, recursion, parallelization, variables called x and more than 10... of anything Comments should usually be more than just the code in other words Good variable names and well-written code doesn't always need comments Code pushes should have prefixes: FIX\\#nn merges which fix GitHub issues (#nn represents the GitHub issue number) FEATURE\\#nn merges which implement GitHub feature requests (#nn represents the GitHub issue number) [DOCS] improvements to documentation Docstrings should look like this for non-trivial methods: def sample_method ( param_1 : str , param_2 : Optional [ int ] = None ) -> bool : \"\"\" A short description of what the method does. Parameters: param_1: string describe this parameter param_2: integer (optional) describe this parameter, if it's optional, what is the default Returns: boolean \"\"\"","title":"Code Style"},{"location":"Contributing%20Guide/Set%20Up/Debian%20%28Ubuntu%29/","text":"Debian/Ubuntu Setting Up 1) Install Python (3.10 recommended) We recommmend using pyenv to install and manage Python environments, particularly in development and test environments. 2) Install pip python3 -m ensurepip --upgrade 3) Install Git sudo apt-get update sudo apt-get install git 4) Clone the Repository git clone https://github.com/mabel-dev/opteryx 5) Install Dependencies python3 -m pip install --upgrade -r requirements.txt 6) Build Binaries python3 setup.py build_ext --inplace Running Tests To run the regression and unit tests: python3 -m pytest","title":"Debian/Ubuntu"},{"location":"Contributing%20Guide/Set%20Up/Debian%20%28Ubuntu%29/#debianubuntu","text":"","title":"Debian/Ubuntu"},{"location":"Contributing%20Guide/Set%20Up/Debian%20%28Ubuntu%29/#setting-up","text":"","title":"Setting Up"},{"location":"Contributing%20Guide/Set%20Up/Debian%20%28Ubuntu%29/#1-install-python-310-recommended","text":"We recommmend using pyenv to install and manage Python environments, particularly in development and test environments.","title":"1) Install Python (3.10 recommended)"},{"location":"Contributing%20Guide/Set%20Up/Debian%20%28Ubuntu%29/#2-install-pip","text":"python3 -m ensurepip --upgrade","title":"2) Install pip"},{"location":"Contributing%20Guide/Set%20Up/Debian%20%28Ubuntu%29/#3-install-git","text":"sudo apt-get update sudo apt-get install git","title":"3) Install Git"},{"location":"Contributing%20Guide/Set%20Up/Debian%20%28Ubuntu%29/#4-clone-the-repository","text":"git clone https://github.com/mabel-dev/opteryx","title":"4) Clone the Repository"},{"location":"Contributing%20Guide/Set%20Up/Debian%20%28Ubuntu%29/#5-install-dependencies","text":"python3 -m pip install --upgrade -r requirements.txt","title":"5) Install Dependencies"},{"location":"Contributing%20Guide/Set%20Up/Debian%20%28Ubuntu%29/#6-build-binaries","text":"python3 setup.py build_ext --inplace","title":"6) Build Binaries"},{"location":"Contributing%20Guide/Set%20Up/Debian%20%28Ubuntu%29/#running-tests","text":"To run the regression and unit tests: python3 -m pytest","title":"Running Tests"},{"location":"Contributing%20Guide/Set%20Up/MacOS/","text":"MacOS Setting Up 1) Install Python (3.10 recommended) We recommmend using pyenv to install and manage Python environments, particularly in development and test environments. 2) Install pip python3 -m ensurepip --upgrade 3) Install Git sudo apt-get update sudo apt-get install git 4) Clone the Repository git clone https://github.com/mabel-dev/opteryx 5) Install Dependencies python3 -m pip install --upgrade -r requirements.txt 6) Build Binaries python3 setup.py build_ext --inplace Running Tests To run the regression and unit tests: python3 -m pytest","title":"MacOS"},{"location":"Contributing%20Guide/Set%20Up/MacOS/#macos","text":"","title":"MacOS"},{"location":"Contributing%20Guide/Set%20Up/MacOS/#setting-up","text":"","title":"Setting Up"},{"location":"Contributing%20Guide/Set%20Up/MacOS/#1-install-python-310-recommended","text":"We recommmend using pyenv to install and manage Python environments, particularly in development and test environments.","title":"1) Install Python (3.10 recommended)"},{"location":"Contributing%20Guide/Set%20Up/MacOS/#2-install-pip","text":"python3 -m ensurepip --upgrade","title":"2) Install pip"},{"location":"Contributing%20Guide/Set%20Up/MacOS/#3-install-git","text":"sudo apt-get update sudo apt-get install git","title":"3) Install Git"},{"location":"Contributing%20Guide/Set%20Up/MacOS/#4-clone-the-repository","text":"git clone https://github.com/mabel-dev/opteryx","title":"4) Clone the Repository"},{"location":"Contributing%20Guide/Set%20Up/MacOS/#5-install-dependencies","text":"python3 -m pip install --upgrade -r requirements.txt","title":"5) Install Dependencies"},{"location":"Contributing%20Guide/Set%20Up/MacOS/#6-build-binaries","text":"python3 setup.py build_ext --inplace","title":"6) Build Binaries"},{"location":"Contributing%20Guide/Set%20Up/MacOS/#running-tests","text":"To run the regression and unit tests: python3 -m pytest","title":"Running Tests"},{"location":"Contributing%20Guide/Set%20Up/Windows/","text":"Windows For WSL, see the Debian/Ubuntu set up guide. Setting Up 1) Install Python (3.10 recommended) We recommmend using pyenv to install and manage Python environments, particularly in development and test environments. 2) Install pip python -m ensurepip --upgrade 3) Install Git sudo apt-get update sudo apt-get install git 4) Clone the Repository git clone https://github.com/mabel-dev/opteryx 5) Install Dependencies python -m pip install --upgrade -r requirements.txt 6) Build Binaries python setup.py build_ext --inplace Running Tests To run the regression and unit tests: python -m pytest","title":"Windows"},{"location":"Contributing%20Guide/Set%20Up/Windows/#windows","text":"For WSL, see the Debian/Ubuntu set up guide.","title":"Windows"},{"location":"Contributing%20Guide/Set%20Up/Windows/#setting-up","text":"","title":"Setting Up"},{"location":"Contributing%20Guide/Set%20Up/Windows/#1-install-python-310-recommended","text":"We recommmend using pyenv to install and manage Python environments, particularly in development and test environments.","title":"1) Install Python (3.10 recommended)"},{"location":"Contributing%20Guide/Set%20Up/Windows/#2-install-pip","text":"python -m ensurepip --upgrade","title":"2) Install pip"},{"location":"Contributing%20Guide/Set%20Up/Windows/#3-install-git","text":"sudo apt-get update sudo apt-get install git","title":"3) Install Git"},{"location":"Contributing%20Guide/Set%20Up/Windows/#4-clone-the-repository","text":"git clone https://github.com/mabel-dev/opteryx","title":"4) Clone the Repository"},{"location":"Contributing%20Guide/Set%20Up/Windows/#5-install-dependencies","text":"python -m pip install --upgrade -r requirements.txt","title":"5) Install Dependencies"},{"location":"Contributing%20Guide/Set%20Up/Windows/#6-build-binaries","text":"python setup.py build_ext --inplace","title":"6) Build Binaries"},{"location":"Contributing%20Guide/Set%20Up/Windows/#running-tests","text":"To run the regression and unit tests: python -m pytest","title":"Running Tests"},{"location":"Deployment/01%20Configuration/","text":"Configuration Configuration File Configuration values are set a opteryx.yaml file in the directory the application is run from. Key Default Description INTERNAL_BATCH_SIZE 500 Batch size for left-table of a join processes MAX_JOIN_SIZE 1000000 Maximum records created in a CROSS JOIN frame MEMCACHED_SERVER not set Address of Memcached server, in IP:PORT format MAX_SUB_PROCESSES Physical CPU count Subprocesses used to parallelize processing BUFFER_PER_SUB_PROCESS 100000000 Memory to allocate per subprocess MAXIMUM_SECONDS_SUB_PROCESSES_CAN_RUN 3600 Time to wait before killing subprocesses DATASET_PREFIX_MAPPING _ reader PARTITION_SCHEME mabel How the blob/file data is partitioned MAX_SIZE_SINGLE_CACHE_ITEM 1048576 The maximum size of an item to store in the buffer cache PAGE_SIZE 67108864 The size to try to make data pages as they are processed Environment Variables The environment is the preferred location for secrets, although the engine will read .env files if dotenv is installed. MONGO_CONNECTION MINIO_END_POINT MINIO_ACCESS_KEY MINIO_SECRET_KEY MINIO_SECURE","title":"Configuration"},{"location":"Deployment/01%20Configuration/#configuration","text":"","title":"Configuration"},{"location":"Deployment/01%20Configuration/#configuration-file","text":"Configuration values are set a opteryx.yaml file in the directory the application is run from. Key Default Description INTERNAL_BATCH_SIZE 500 Batch size for left-table of a join processes MAX_JOIN_SIZE 1000000 Maximum records created in a CROSS JOIN frame MEMCACHED_SERVER not set Address of Memcached server, in IP:PORT format MAX_SUB_PROCESSES Physical CPU count Subprocesses used to parallelize processing BUFFER_PER_SUB_PROCESS 100000000 Memory to allocate per subprocess MAXIMUM_SECONDS_SUB_PROCESSES_CAN_RUN 3600 Time to wait before killing subprocesses DATASET_PREFIX_MAPPING _ reader PARTITION_SCHEME mabel How the blob/file data is partitioned MAX_SIZE_SINGLE_CACHE_ITEM 1048576 The maximum size of an item to store in the buffer cache PAGE_SIZE 67108864 The size to try to make data pages as they are processed","title":"Configuration File"},{"location":"Deployment/01%20Configuration/#environment-variables","text":"The environment is the preferred location for secrets, although the engine will read .env files if dotenv is installed. MONGO_CONNECTION MINIO_END_POINT MINIO_ACCESS_KEY MINIO_SECRET_KEY MINIO_SECURE","title":"Environment Variables"},{"location":"Deployment/02%20Caching/","text":"Caching Buffer Pool Read cache, saving a copy of data stored remotely to a faster store. In Memory Cache Uses the cache local to the machine to cache pages. Fastest, but most limiting and volatile. Memcached Cache Uses a Memcached instance to cache pages. Is a good option when remote reads are slow, for example from GCS or S3.","title":"Caching"},{"location":"Deployment/02%20Caching/#caching","text":"","title":"Caching"},{"location":"Deployment/02%20Caching/#buffer-pool","text":"Read cache, saving a copy of data stored remotely to a faster store.","title":"Buffer Pool"},{"location":"Deployment/02%20Caching/#in-memory-cache","text":"Uses the cache local to the machine to cache pages. Fastest, but most limiting and volatile.","title":"In Memory Cache"},{"location":"Deployment/02%20Caching/#memcached-cache","text":"Uses a Memcached instance to cache pages. Is a good option when remote reads are slow, for example from GCS or S3.","title":"Memcached Cache"},{"location":"Deployment/02%20Storage/","text":"Storage Folder Structure Datasets Opteryx references datasets using their relative path as the table name. For example in the following folder structure / \u251c\u2500\u2500 products/ \u251c\u2500\u2500 customers/ \u2502 \u251c\u2500\u2500 profiles/ \u2502 \u2514\u2500\u2500 preferences/ \u2502 \u251c\u2500\u2500 marketing/ \u2502 \u2514\u2500\u2500 site/ \u2514\u2500\u2500 purchases/ Would have the following datasets available (assuming leaf folders had data files within them) products customers.profiles customers.preferences.marketing customers.preferences.site purchases Temporal Structures To enable temporal queries, data must be structured into date hierachy folders below the dataset folder. Using just the products dataset from the above example, below the products folder must be year, month and day folders like this: / \u2514\u2500\u2500 products/ \u2514\u2500\u2500 year_2022/ \u2514\u2500\u2500 month_05/ \u2514\u2500\u2500 day_01/ To query the data for today with this structure, you can execute: SELECT * FROM products To query just the folder shown in the example (1st May 2022), you can execute: SELECT * FROM products FOR '2022-05-01' This is the default structure created by Mabel and within Opteryx this is called Mabel Partitioning. File Types Opteryx is primarily designed for use with Parquet to store data, Parquet is fast to process and offers optimizations not available for other formats, however, in some benchmarks ORC out performs Parquet. Opteryx also supports JSONL files, and JSONL files which have been Zstandard compressed ( .zstd ). .jsonl and .zstd format files are the default storage for Mabel . Opteryx also has support for Feather (Arrow) files. File Sizes Opteryx loads entire files (pages) into memory one at a time, this requires the following to be considered: Reading one record from a file loads the entire page. If you regularly only read a few records, prefer smaller pages. Reading each page, particularly from Cloud Storage (S3/GCS), incurs a per-read overhead. If you have large datasets, prefer larger pages. If you are unsure where to start, 64Mb (before compression) is a good general-purpose page size.","title":"Storage"},{"location":"Deployment/02%20Storage/#storage","text":"","title":"Storage"},{"location":"Deployment/02%20Storage/#folder-structure","text":"","title":"Folder Structure"},{"location":"Deployment/02%20Storage/#datasets","text":"Opteryx references datasets using their relative path as the table name. For example in the following folder structure / \u251c\u2500\u2500 products/ \u251c\u2500\u2500 customers/ \u2502 \u251c\u2500\u2500 profiles/ \u2502 \u2514\u2500\u2500 preferences/ \u2502 \u251c\u2500\u2500 marketing/ \u2502 \u2514\u2500\u2500 site/ \u2514\u2500\u2500 purchases/ Would have the following datasets available (assuming leaf folders had data files within them) products customers.profiles customers.preferences.marketing customers.preferences.site purchases","title":"Datasets"},{"location":"Deployment/02%20Storage/#temporal-structures","text":"To enable temporal queries, data must be structured into date hierachy folders below the dataset folder. Using just the products dataset from the above example, below the products folder must be year, month and day folders like this: / \u2514\u2500\u2500 products/ \u2514\u2500\u2500 year_2022/ \u2514\u2500\u2500 month_05/ \u2514\u2500\u2500 day_01/ To query the data for today with this structure, you can execute: SELECT * FROM products To query just the folder shown in the example (1st May 2022), you can execute: SELECT * FROM products FOR '2022-05-01' This is the default structure created by Mabel and within Opteryx this is called Mabel Partitioning.","title":"Temporal Structures"},{"location":"Deployment/02%20Storage/#file-types","text":"Opteryx is primarily designed for use with Parquet to store data, Parquet is fast to process and offers optimizations not available for other formats, however, in some benchmarks ORC out performs Parquet. Opteryx also supports JSONL files, and JSONL files which have been Zstandard compressed ( .zstd ). .jsonl and .zstd format files are the default storage for Mabel . Opteryx also has support for Feather (Arrow) files.","title":"File Types"},{"location":"Deployment/02%20Storage/#file-sizes","text":"Opteryx loads entire files (pages) into memory one at a time, this requires the following to be considered: Reading one record from a file loads the entire page. If you regularly only read a few records, prefer smaller pages. Reading each page, particularly from Cloud Storage (S3/GCS), incurs a per-read overhead. If you have large datasets, prefer larger pages. If you are unsure where to start, 64Mb (before compression) is a good general-purpose page size.","title":"File Sizes"},{"location":"Deployment/04%20Hosting/","text":"Hosting Host Systems Python version 3.10 on Linux (Debian/Ubuntu) is the recommended hosting environment for Opteryx. Opteryx has builds for Python 3.8, 3.9 and 3.10 on 64-bit versions of Windows, MacOS and Linux. The full regession suite is run on Ubuntu (Ubuntu 20.04) for Python version 3.8, 3.9 and 3.10. Docker Google Cloud Cloud Run Running in the Generation 2 container environment is likely to result in faster query processing, but has a slower start-up time. Opteryx runs in Generation 1 container, taking approximately 10% longer to execute queries. Note that Opteryx is does not make use of multiple CPUs, although multiple CPUs may be beneficial to allow higher memory allocations.","title":"Hosting"},{"location":"Deployment/04%20Hosting/#hosting","text":"","title":"Hosting"},{"location":"Deployment/04%20Hosting/#host-systems","text":"Python version 3.10 on Linux (Debian/Ubuntu) is the recommended hosting environment for Opteryx. Opteryx has builds for Python 3.8, 3.9 and 3.10 on 64-bit versions of Windows, MacOS and Linux. The full regession suite is run on Ubuntu (Ubuntu 20.04) for Python version 3.8, 3.9 and 3.10.","title":"Host Systems"},{"location":"Deployment/04%20Hosting/#docker","text":"","title":"Docker"},{"location":"Deployment/04%20Hosting/#google-cloud","text":"Cloud Run Running in the Generation 2 container environment is likely to result in faster query processing, but has a slower start-up time. Opteryx runs in Generation 1 container, taking approximately 10% longer to execute queries. Note that Opteryx is does not make use of multiple CPUs, although multiple CPUs may be beneficial to allow higher memory allocations.","title":"Google Cloud"},{"location":"Deployment/Internals/Query%20Engine/01%20Overview/","text":"Query Engine Tokenize, Parse, Lex, AST Plan Execute","title":"Query Engine"},{"location":"Deployment/Internals/Query%20Engine/01%20Overview/#query-engine","text":"Tokenize, Parse, Lex, AST Plan Execute","title":"Query Engine"},{"location":"Deployment/Internals/Query%20Engine/01%20Parser/","text":"Parser MySQL flavoured parser sqloxide Python bindings for sqlparser-rs","title":"Parser"},{"location":"Deployment/Internals/Query%20Engine/01%20Parser/#parser","text":"MySQL flavoured parser sqloxide Python bindings for sqlparser-rs","title":"Parser"},{"location":"Deployment/Internals/Query%20Engine/02%20Planner/","text":"Query Planner Query Plan Steps Query Plans can contain the following steps: Step Description Aggregate Perform aggregations such as COUNT and MAX Distinct Remove duplicate records Evaluation Evaluate functions Explain Plan Explainer Join Join Relations Limit Return up to a stated number of records Offset Skip a number of records Projection Remove unwanted columns Reader Read datasets Selection Remove unwanted rows Query Plan Optimizer The Query Plan is naive and performs no optimizations.","title":"Query Planner"},{"location":"Deployment/Internals/Query%20Engine/02%20Planner/#query-planner","text":"","title":"Query Planner"},{"location":"Deployment/Internals/Query%20Engine/02%20Planner/#query-plan-steps","text":"Query Plans can contain the following steps: Step Description Aggregate Perform aggregations such as COUNT and MAX Distinct Remove duplicate records Evaluation Evaluate functions Explain Plan Explainer Join Join Relations Limit Return up to a stated number of records Offset Skip a number of records Projection Remove unwanted columns Reader Read datasets Selection Remove unwanted rows","title":"Query Plan Steps"},{"location":"Deployment/Internals/Query%20Engine/02%20Planner/#query-plan-optimizer","text":"The Query Plan is naive and performs no optimizations.","title":"Query Plan Optimizer"},{"location":"Deployment/Internals/Query%20Engine/03%20Execution/","text":"Query Execution Overview The planner creates a plan as a DAG, we can see simple plans using the EXPLAIN keyword, this is added as the first clause in a statement and the planner will return the plan for the query. To see the plan for this query SELECT * FROM $ planets ; We insert the EXPLAIN clause EXPLAIN SELECT * FROM $ planets ; Which returns Note that the query plan order does not match the order that the query is written, instead it runs in the order required to respond to the query. The execution engine starts at the node at left of the tree (LIMIT 10) and requests records from the node below (SELECT *), which in turn requests records from the node to the right (WHERE Alive = TRUE). This continues until we reach a node which can feed data into the tree, this will usually be at a node which reads data files (FROM data_table). Records are read in batches, and processed in batches, so although the leftmost node will emit it's result when it has 10 records, it may have recieved 1000 records. These batches are the data frames with broadly align to the database concept of data pages. Execution Optimization Two optimizations are used to improve execution performance: Processing of data is done by page-by-page Vectorization of aggregation and function calculations","title":"Query Execution"},{"location":"Deployment/Internals/Query%20Engine/03%20Execution/#query-execution","text":"","title":"Query Execution"},{"location":"Deployment/Internals/Query%20Engine/03%20Execution/#overview","text":"The planner creates a plan as a DAG, we can see simple plans using the EXPLAIN keyword, this is added as the first clause in a statement and the planner will return the plan for the query. To see the plan for this query SELECT * FROM $ planets ; We insert the EXPLAIN clause EXPLAIN SELECT * FROM $ planets ; Which returns Note that the query plan order does not match the order that the query is written, instead it runs in the order required to respond to the query. The execution engine starts at the node at left of the tree (LIMIT 10) and requests records from the node below (SELECT *), which in turn requests records from the node to the right (WHERE Alive = TRUE). This continues until we reach a node which can feed data into the tree, this will usually be at a node which reads data files (FROM data_table). Records are read in batches, and processed in batches, so although the leftmost node will emit it's result when it has 10 records, it may have recieved 1000 records. These batches are the data frames with broadly align to the database concept of data pages.","title":"Overview"},{"location":"Deployment/Internals/Query%20Engine/03%20Execution/#execution-optimization","text":"Two optimizations are used to improve execution performance: Processing of data is done by page-by-page Vectorization of aggregation and function calculations","title":"Execution Optimization"},{"location":"Deployment/Internals/Storage%20Engine/01%20Storage%20Formats/","text":"Storage Formats Supported Data Files Parquet Parquet offers optimizations not available with other formats which improve query performance. If a datasource has query performance issues or is hot in terms of query use, converting to Parquet is likely to improve performance. Do not take this as true for all situations, do test for your specific circumstances. JSONL JSONL and zStandard compressed JSONL files. Files don't need an explicit schema, but each partition must have the same columns in the same order in every row of every file. Data types are inferred from the records, where data types are not consistent, the read will fail. Opteryx supports zStandard Compressed JSONL files as created by Mabel.","title":"Storage Formats"},{"location":"Deployment/Internals/Storage%20Engine/01%20Storage%20Formats/#storage-formats","text":"","title":"Storage Formats"},{"location":"Deployment/Internals/Storage%20Engine/01%20Storage%20Formats/#supported-data-files","text":"","title":"Supported Data Files"},{"location":"Deployment/Internals/Storage%20Engine/01%20Storage%20Formats/#parquet","text":"Parquet offers optimizations not available with other formats which improve query performance. If a datasource has query performance issues or is hot in terms of query use, converting to Parquet is likely to improve performance. Do not take this as true for all situations, do test for your specific circumstances.","title":"Parquet"},{"location":"Deployment/Internals/Storage%20Engine/01%20Storage%20Formats/#jsonl","text":"JSONL and zStandard compressed JSONL files. Files don't need an explicit schema, but each partition must have the same columns in the same order in every row of every file. Data types are inferred from the records, where data types are not consistent, the read will fail. Opteryx supports zStandard Compressed JSONL files as created by Mabel.","title":"JSONL"},{"location":"Deployment/Internals/Storage%20Engine/02%20Storage%20Layout/","text":"Storage Layout Flat dataset file_1 file_2 file_3 Mabel dataset year_2020 month_03 day_04 file_1 file_2 file_3 Mabel structured data enables temporal queries.","title":"Storage Layout"},{"location":"Deployment/Internals/Storage%20Engine/02%20Storage%20Layout/#storage-layout","text":"","title":"Storage Layout"},{"location":"Deployment/Internals/Storage%20Engine/02%20Storage%20Layout/#flat","text":"dataset file_1 file_2 file_3","title":"Flat"},{"location":"Deployment/Internals/Storage%20Engine/02%20Storage%20Layout/#mabel","text":"dataset year_2020 month_03 day_04 file_1 file_2 file_3 Mabel structured data enables temporal queries.","title":"Mabel"},{"location":"Deployment/Internals/Storage%20Engine/03%20Storage%20Adapters/","text":"Storage Adapters Local Disk Network Google Cloud Storage AWS S3 (Minio)","title":"Storage Adapters"},{"location":"Deployment/Internals/Storage%20Engine/03%20Storage%20Adapters/#storage-adapters","text":"","title":"Storage Adapters"},{"location":"Deployment/Internals/Storage%20Engine/03%20Storage%20Adapters/#local","text":"","title":"Local"},{"location":"Deployment/Internals/Storage%20Engine/03%20Storage%20Adapters/#disk","text":"","title":"Disk"},{"location":"Deployment/Internals/Storage%20Engine/03%20Storage%20Adapters/#network","text":"","title":"Network"},{"location":"Deployment/Internals/Storage%20Engine/03%20Storage%20Adapters/#google-cloud-storage","text":"","title":"Google Cloud Storage"},{"location":"Deployment/Internals/Storage%20Engine/03%20Storage%20Adapters/#aws-s3-minio","text":"","title":"AWS S3 (Minio)"},{"location":"Features/Multi%20Platform/","text":"Multi Platform Runs pretty much anywhere you can run Python Designed to run in Knative containers, Opteryx can run almost anywhere you can run Python. docker K8s Knative Supports multiple data stores Local Storage Google Cloud Storage MinIo/S3 MongoDB Firestore","title":"Multi Platform"},{"location":"Features/Multi%20Platform/#multi-platform","text":"","title":"Multi Platform"},{"location":"Features/Multi%20Platform/#runs-pretty-much-anywhere-you-can-run-python","text":"Designed to run in Knative containers, Opteryx can run almost anywhere you can run Python. docker K8s Knative","title":"Runs pretty much anywhere you can run Python"},{"location":"Features/Multi%20Platform/#supports-multiple-data-stores","text":"Local Storage Google Cloud Storage MinIo/S3 MongoDB Firestore","title":"Supports multiple data stores"},{"location":"Features/Schema%20Evolution/","text":"Schema Evolution Opteryx has support for in-place relation evolution. You can evolve a table schema or change a partition layout without requiring existing data to be rewritten or migrated to a new dataset. Schema of the data is determined by the first page read to respond to a query, new columns are removed and missing columns are null-filled. This allows graceful handling of pages with different schemas, but may result in the appearance of missing data as columns not found in the first page are removed. Opteryx supports the following schema evolution changes: Add - new columns can be added - these are removed if not present on the first page read Remove - removed columns are null-filled Reorder - the order of columns can be changed Partitioning - partition resolution can be changed Note Renamed columns will behave like the column has been removed and a new column added. Partitioning Changes to partition schemes are handled transparently. For example, data using Mabel partitioning moving from a daily to an hourly partition layout can occur without requiring any other changes to the configuration of the query engine. However, moving between no partition and partitioning (or vise-versa) is not supported.","title":"Schema Evolution"},{"location":"Features/Schema%20Evolution/#schema-evolution","text":"Opteryx has support for in-place relation evolution. You can evolve a table schema or change a partition layout without requiring existing data to be rewritten or migrated to a new dataset. Schema of the data is determined by the first page read to respond to a query, new columns are removed and missing columns are null-filled. This allows graceful handling of pages with different schemas, but may result in the appearance of missing data as columns not found in the first page are removed. Opteryx supports the following schema evolution changes: Add - new columns can be added - these are removed if not present on the first page read Remove - removed columns are null-filled Reorder - the order of columns can be changed Partitioning - partition resolution can be changed Note Renamed columns will behave like the column has been removed and a new column added.","title":"Schema Evolution"},{"location":"Features/Schema%20Evolution/#partitioning","text":"Changes to partition schemes are handled transparently. For example, data using Mabel partitioning moving from a daily to an hourly partition layout can occur without requiring any other changes to the configuration of the query engine. However, moving between no partition and partitioning (or vise-versa) is not supported.","title":"Partitioning"},{"location":"Features/Time%20Travel/","text":"Time Travel Opteryx","title":"Time Travel"},{"location":"Features/Time%20Travel/#time-travel","text":"Opteryx","title":"Time Travel"},{"location":"Release%20Notes/Change%20Log/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [Unreleased] Added [ #232 ] Support DATEPART and EXTRACT date functions. ( @joocer ) [ #63 ] Estimate row counts when reading blobs. ( @joocer ) [ #231 ] Implement DATEDIFF function. ( @joocer ) [ #301 ] Optimizations for IS conditions. ( @joocer ) [ #229 ] Support TIME_BUCKET function. ( @joocer ) Changed [ #35 ] Table scan planning done during query planning. ( @joocer ) [ #173 ] Data not found raises different errors under different scenarios. ( @joocer ) Implementation of LEFT and RIGHT functions to reduce execution time. ( @joocer ) [ #258 ] Code release approach. ( @joocer ) [ #295 ] Removed redundant projection when SELECT * . ( @joocer ) [ #297 ] Filters on SHOW COLUMNS execute before profiling. ( @joocer ) Fixed [ #252 ] Planner should gracefully convert byte strings to ascii strings. ( @joocer ) [ #184 ] Schema changes cause unexpected and unhelpful failures. ( @joocer ) [ #261 ] Read fails if buffer cache is unavailable. ( @joocer ) [ #277 ] Cache errors should be transparent. ( @joocer ) [ #285 ] DISTINCT on nulls throws error. ( @joocer ) [ #281 ] SELECT on empty aggregates reports missing columns. ( @joocer ) [ #312 ] Invalid dates in FOR clauses treated as TODAY . ( @joocer ) [0.1.0] - 2022-07-02 Added [ #165 ] Support S3/MinIO data stores for blobs. ( @joocer ) FAKE dataset constructor (part of #179 ). ( @joocer ) [ #177 ] Support SHOW FULL COLUMNS to read entire datasets rather than just the first blob. ( @joocer ) [ #194 ] Functions that are abbreviations, should have the full name as an alias. ( @joocer ) [ #201 ] generate_series supports CIDR expansion. ( @joocer ) [ #175 ] Support WITH (NOCACHE) hint to disable using cache. ( @joocer ) [ #203 ] When reporting that a column doesn't exist, it should suggest likely correct columns. ( @joocer ) Not Regular Expression match operator, !~ added to supported set of operators. ( @joocer ) [ #226 ] Implement DATE_TRUNC function. ( @joocer ) [ #230 ] Allow addressing fields as numbers. ( @joocer ) [ #234 ] Implement SEARCH function. ( @joocer ) [ #237 ] Implement COALESCE function. ( @joocer ) Changed (non-breaking) Blob-based readers (disk & GCS) moved from 'local' and 'network' paths to a new 'blob' path. ( @joocer ) (non-breaking) Query Execution rewritten. ( @joocer ) [ #20 ] Split query planner and query plan into different modules. ( @joocer ) [ #164 ] Split dataset reader into specific types. ( @joocer ) Expression evaluation short-cuts execution when executing evaluations against an array of null . ( @joocer ) [ #244 ] Improve performance of IN test against literal lists. ( @joocer ) Fixed [ #172 ] LIKE on non string column gives confusing error ( @joocer ) [ #179 ] Aggregate Node creates new metadata for each chunk ( @joocer ) [ #183 ] NOT doesn't display in plan correctly ( @joocer ) [ #182 ] Unable to evaluate valid filters ( @joocer ) [ #178 ] SHOW COLUMNS returns type OTHER when it can probably work out the type ( @joocer ) [ #128 ] JOIN fails, using PyArrow .join() ( @joocer ) [ #189 ] Explicit JOIN algorithm exceeds memory ( @joocer ) [ #199 ] SHOW EXTENDED COLUMNS blows memory allocations on large tables ( @joocer ) [ #169 ] Selection nodes in EXPLAIN have nested parentheses. ( @joocer ) [ #220 ] LIKE clause fails for columns that contain nulls. ( @joocer ) [ #222 ] Column of NULL detects as VARCHAR . ( @joocer ) [ #225 ] UNNEST does not assign a type to the column when all of the values are NULL . ( @joocer ) [0.0.2] - 2022-06-03 Added [ #72 ] Configuration is now read from opteryx.yaml rather than the environment. ( @joocer ) [ #139 ] Gather statistics on planning reading of segements. ( @joocer ) [ #151 ] Implement SELECT table.* . ( @joocer ) [ #137 ] GENERATE_SERIES function. ( @joocer ) Fixed [ #106 ] ORDER BY on qualified fields fails ( @joocer ) [ #103 ] ORDER BY after JOIN errors ( @joocer ) [ #110 ] SubQueries AS statement ignored ( @joocer ) [ #112 ] SHOW COLUMNS doesn't work for non sample datasets ( @joocer ) [ #113 ] Sample data has NaN as a string, rather than the value ( @joocer ) [ #111 ] CROSS JOIN UNNEST should return a NONE when the list is empty (or NONE ) ( @joocer ) [ #119 ] 'NoneType' object is not iterable error on UNNEST ( @joocer ) [ #127 ] Reading from segments appears to only read the first segment ( @joocer ) [ #132 ] Multiprocessing regressed Caching functionality ( @joocer ) [ #140 ] Appears to have read both frames rather than the latest frame ( @joocer ) [ #144 ] Multiple JOINS in one query aren't recognized ( @joocer ) [0.0.1] - 2022-05-09 Added Additional statistics recording the time taken to scan partitions ( @joocer ) Support for FULL JOIN and RIGHT JOIN ( @joocer ) Changed (non-breaking) Use PyArrow implementation for INNER JOIN and LEFT JOIN ( @joocer ) Fixed [ #99 ] Grouping by a list gives an unhelpful error message ( @joocer ) [ #100 ] Projection ignores field qualifications ( @joocer ) [0.0.0] Initial Version","title":"Changelog"},{"location":"Release%20Notes/Change%20Log/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"Release%20Notes/Change%20Log/#unreleased","text":"Added [ #232 ] Support DATEPART and EXTRACT date functions. ( @joocer ) [ #63 ] Estimate row counts when reading blobs. ( @joocer ) [ #231 ] Implement DATEDIFF function. ( @joocer ) [ #301 ] Optimizations for IS conditions. ( @joocer ) [ #229 ] Support TIME_BUCKET function. ( @joocer ) Changed [ #35 ] Table scan planning done during query planning. ( @joocer ) [ #173 ] Data not found raises different errors under different scenarios. ( @joocer ) Implementation of LEFT and RIGHT functions to reduce execution time. ( @joocer ) [ #258 ] Code release approach. ( @joocer ) [ #295 ] Removed redundant projection when SELECT * . ( @joocer ) [ #297 ] Filters on SHOW COLUMNS execute before profiling. ( @joocer ) Fixed [ #252 ] Planner should gracefully convert byte strings to ascii strings. ( @joocer ) [ #184 ] Schema changes cause unexpected and unhelpful failures. ( @joocer ) [ #261 ] Read fails if buffer cache is unavailable. ( @joocer ) [ #277 ] Cache errors should be transparent. ( @joocer ) [ #285 ] DISTINCT on nulls throws error. ( @joocer ) [ #281 ] SELECT on empty aggregates reports missing columns. ( @joocer ) [ #312 ] Invalid dates in FOR clauses treated as TODAY . ( @joocer )","title":"[Unreleased]"},{"location":"Release%20Notes/Change%20Log/#010-2022-07-02","text":"Added [ #165 ] Support S3/MinIO data stores for blobs. ( @joocer ) FAKE dataset constructor (part of #179 ). ( @joocer ) [ #177 ] Support SHOW FULL COLUMNS to read entire datasets rather than just the first blob. ( @joocer ) [ #194 ] Functions that are abbreviations, should have the full name as an alias. ( @joocer ) [ #201 ] generate_series supports CIDR expansion. ( @joocer ) [ #175 ] Support WITH (NOCACHE) hint to disable using cache. ( @joocer ) [ #203 ] When reporting that a column doesn't exist, it should suggest likely correct columns. ( @joocer ) Not Regular Expression match operator, !~ added to supported set of operators. ( @joocer ) [ #226 ] Implement DATE_TRUNC function. ( @joocer ) [ #230 ] Allow addressing fields as numbers. ( @joocer ) [ #234 ] Implement SEARCH function. ( @joocer ) [ #237 ] Implement COALESCE function. ( @joocer ) Changed (non-breaking) Blob-based readers (disk & GCS) moved from 'local' and 'network' paths to a new 'blob' path. ( @joocer ) (non-breaking) Query Execution rewritten. ( @joocer ) [ #20 ] Split query planner and query plan into different modules. ( @joocer ) [ #164 ] Split dataset reader into specific types. ( @joocer ) Expression evaluation short-cuts execution when executing evaluations against an array of null . ( @joocer ) [ #244 ] Improve performance of IN test against literal lists. ( @joocer ) Fixed [ #172 ] LIKE on non string column gives confusing error ( @joocer ) [ #179 ] Aggregate Node creates new metadata for each chunk ( @joocer ) [ #183 ] NOT doesn't display in plan correctly ( @joocer ) [ #182 ] Unable to evaluate valid filters ( @joocer ) [ #178 ] SHOW COLUMNS returns type OTHER when it can probably work out the type ( @joocer ) [ #128 ] JOIN fails, using PyArrow .join() ( @joocer ) [ #189 ] Explicit JOIN algorithm exceeds memory ( @joocer ) [ #199 ] SHOW EXTENDED COLUMNS blows memory allocations on large tables ( @joocer ) [ #169 ] Selection nodes in EXPLAIN have nested parentheses. ( @joocer ) [ #220 ] LIKE clause fails for columns that contain nulls. ( @joocer ) [ #222 ] Column of NULL detects as VARCHAR . ( @joocer ) [ #225 ] UNNEST does not assign a type to the column when all of the values are NULL . ( @joocer )","title":"[0.1.0] - 2022-07-02"},{"location":"Release%20Notes/Change%20Log/#002-2022-06-03","text":"Added [ #72 ] Configuration is now read from opteryx.yaml rather than the environment. ( @joocer ) [ #139 ] Gather statistics on planning reading of segements. ( @joocer ) [ #151 ] Implement SELECT table.* . ( @joocer ) [ #137 ] GENERATE_SERIES function. ( @joocer ) Fixed [ #106 ] ORDER BY on qualified fields fails ( @joocer ) [ #103 ] ORDER BY after JOIN errors ( @joocer ) [ #110 ] SubQueries AS statement ignored ( @joocer ) [ #112 ] SHOW COLUMNS doesn't work for non sample datasets ( @joocer ) [ #113 ] Sample data has NaN as a string, rather than the value ( @joocer ) [ #111 ] CROSS JOIN UNNEST should return a NONE when the list is empty (or NONE ) ( @joocer ) [ #119 ] 'NoneType' object is not iterable error on UNNEST ( @joocer ) [ #127 ] Reading from segments appears to only read the first segment ( @joocer ) [ #132 ] Multiprocessing regressed Caching functionality ( @joocer ) [ #140 ] Appears to have read both frames rather than the latest frame ( @joocer ) [ #144 ] Multiple JOINS in one query aren't recognized ( @joocer )","title":"[0.0.2] - 2022-06-03"},{"location":"Release%20Notes/Change%20Log/#001-2022-05-09","text":"Added Additional statistics recording the time taken to scan partitions ( @joocer ) Support for FULL JOIN and RIGHT JOIN ( @joocer ) Changed (non-breaking) Use PyArrow implementation for INNER JOIN and LEFT JOIN ( @joocer ) Fixed [ #99 ] Grouping by a list gives an unhelpful error message ( @joocer ) [ #100 ] Projection ignores field qualifications ( @joocer )","title":"[0.0.1] - 2022-05-09"},{"location":"Release%20Notes/Change%20Log/#000","text":"Initial Version","title":"[0.0.0]"},{"location":"Release%20Notes/Notices/","text":"Notices Opteryx uses the following libraries and components: Component Disposition Copyright Licence cityhash Installed . Bespoke cython Installed . Apache 2.0 datetime_truncate Integrated 2020 Media Pop MIT distogram Integrated 2020 by Romain Picard MIT mbleven Integrated 2018 Fujimoto Seiji Public Domain numpy Installed . BSD-3 orjson Installed . Apache 2.0 pyarrow Installed . Apache 2.0 pyarrow_ops Integrated TomScheffers (assumed) Apache 2.0 pyyaml Installed . MIT sqloxide Installed . MIT sqlparser-rs Transitive . Apache 2.0 Installed components are installed from PyPI Integrated components have their source code included in the Opteryx codebase Transitive components are key components installed by other components - this is not intended to be a complete list Note License information was correct as at 2022-06-03","title":"Notices"},{"location":"Release%20Notes/Notices/#notices","text":"Opteryx uses the following libraries and components: Component Disposition Copyright Licence cityhash Installed . Bespoke cython Installed . Apache 2.0 datetime_truncate Integrated 2020 Media Pop MIT distogram Integrated 2020 by Romain Picard MIT mbleven Integrated 2018 Fujimoto Seiji Public Domain numpy Installed . BSD-3 orjson Installed . Apache 2.0 pyarrow Installed . Apache 2.0 pyarrow_ops Integrated TomScheffers (assumed) Apache 2.0 pyyaml Installed . MIT sqloxide Installed . MIT sqlparser-rs Transitive . Apache 2.0 Installed components are installed from PyPI Integrated components have their source code included in the Opteryx codebase Transitive components are key components installed by other components - this is not intended to be a complete list Note License information was correct as at 2022-06-03","title":"Notices"},{"location":"Release%20Notes/Version%20Goals/","text":"Version Goals Version 1.0 Version 1.0 aims to be feature complete for current known use cases. Version 1.0 goals will be delivered across various minor versions building toward v1.0. These minor releases will also include bug fixes, performance improvements and functional completeness. The items listed below are major dev \ud83d\udd32 Planner CTEs ( WITH ) statements supported \u2b1b Planner Read across multiple data sources (e.g. GCS and Postgres in the same query) [v0.2] \u2b1b Execution JOIN statements supported [v0.1] \ud83d\udd32 Execution CASE statements supported \ud83d\udd32 Execution Use asyncio/threading to read data, to improve through-put \ud83d\udd32 Execution Functions using the result of Functions (e.g. LENGTH(LIST(field))) \ud83d\udd32 Execution Inline operators (e.g. firstname || surname)","title":"Version Goals"},{"location":"Release%20Notes/Version%20Goals/#version-goals","text":"","title":"Version Goals"},{"location":"Release%20Notes/Version%20Goals/#version-10","text":"Version 1.0 aims to be feature complete for current known use cases. Version 1.0 goals will be delivered across various minor versions building toward v1.0. These minor releases will also include bug fixes, performance improvements and functional completeness. The items listed below are major dev \ud83d\udd32 Planner CTEs ( WITH ) statements supported \u2b1b Planner Read across multiple data sources (e.g. GCS and Postgres in the same query) [v0.2] \u2b1b Execution JOIN statements supported [v0.1] \ud83d\udd32 Execution CASE statements supported \ud83d\udd32 Execution Use asyncio/threading to read data, to improve through-put \ud83d\udd32 Execution Functions using the result of Functions (e.g. LENGTH(LIST(field))) \ud83d\udd32 Execution Inline operators (e.g. firstname || surname)","title":"Version 1.0"},{"location":"SQL%20Reference/01%20Introduction/","text":"SQL Introduction Overview This page provides an overview of how to perform simple operations in SQL. This tutorial is only intended to give you an introduction and is not a complete tutorial on SQL. This tutorial is reworked from the DuckDB tutorial. All queries use the internal sample NASA datasets and should work regardless of the data your installation and set up has access to. Concepts Opteryx is a system for querying ad hoc data stored in files as relations . A relation is mathematical term for a data table. Each relation is a named collection of rows, organized in columns, each column should be a common datatype. As an ad hoc query engine, the relations and their schema do not need to be predefined, they are determined at the time the query is run. This is one of the reasons Opteryx cannot be considered a RDBMS (relational database management system), even though it can be used to query data using SQL. Querying Relations To retrieve data from a relation, the relation is queried using a SQL SELECT statement. Basic statements are made of three parts; the list of columns to be returned and the list of relations to retrieve data from, and optional clauses to shape and filter the data that is returned. SELECT * FROM $ planets ; The * is shorthand for \"all columns\", by convention keywords are capitalized, and ; optionally terminates the query. SELECT id , name FROM $ planets WHERE name = 'Earth' ; The output of the above query should be id | name ----+------- 3 | Earth You can write functions, not just simple column references, in the select list. For example, you can write: SELECT id , UPPER ( name ) AS uppercase_name FROM $ planets WHERE id = 3 ; This should give: id | uppercase_name ----+---------------- 3 | EARTH Notice how the AS clause is used to relabel the output column. (The AS clause is optional.) A query can be \u201cqualified\u201d by adding a WHERE clause that specifies which rows are wanted. The WHERE clause contains a Boolean (truth value) expression, and only rows for which the Boolean expression is true are returned. The usual Boolean operators ( AND , OR , and NOT ) are allowed in the qualification. For example, the following the planets with fewer than 10 moons and a day longer than 24 hours: SELECT * FROM $ planets WHERE lengthOfDay > 24 AND numberOfMoons < 10 ; Result: name | lengthOfDay | numberOfMoons --------+-------------+--------------- Mercury | 4222.6 | 0 Venus | 2802 | 0 Mars | 24.7 | 2 Pluto | 153.3 | 5 The order of results are not guaranteed and should not be relied upon. If you request the results of the below query, you might get the Mercury or Venus in either order. Note The same query, of the same data in the same version of Opteryx will likely to return results in the same order, don't expect to test result order non-determinism by rerunning the query millions of times and looking for differences. These differences may manifest over different versions, or from subtle differences to the query statement or data. SELECT name , numberOfMoons FROM $ planets WHERE numberOfMoons = 0 ; Result: name | lengthOfDay | numberOfMoons --------+-------------+--------------- Mercury | 4222.6 | 0 Venus | 2802 | 0 But you\u2019d always get the results shown above if you do: SELECT name , numberOfMoons FROM $ planets WHERE numberOfMoons = 0 ORDER BY name ; You can request that duplicate rows be removed from the result of a query: SELECT DISTINCT planetId FROM $ satellites ; Result: planetId -------- 3 4 5 6 7 8 9 Here again, the result row ordering might vary. You can ensure consistent results by using DISTINCT and ORDER BY together: SELECT DISTINCT planetId FROM $ satellites ORDER BY planetId ; Joins Between Relations So far our queries have only accessed one relation at a time. Queries can access multiple relations at once, or access the same relation in such a way that multiple rows of the relation are being processed at the same time. A query that accesses multiple rows of the same or different relations at one time is called a join query. As an example, say you wish to list all the $satellites records together with the planet they orbit. To do that, we need to compare the planetId of each row of the $satellites relation with the id column of all rows in the $planets relation, and return the pairs of rows where these values match. This would be accomplished by the following query: SELECT * FROM $ satellites , $ planets WHERE planetId = $ planets . id ; $satellites.id | planetId | $satellites.name | ... ---------------+----------+------------------+---- 1 | 3 | Moon | 2 | 4 | Phobos | 3 | 4 | Deimos | 4 | 5 | Io | 5 | 5 | Europa | (more rows and columns) Observe two things about the result set: There are no result row for the planets of Mercury or Venus ( planetIds 1 and 2). This is because there is no matching entry in the $satellites relation for these planets, so the join ignores the unmatched rows in the $planets relation. Each of the relations being joined have an id and a name column, to ensure it is clear which relation the value being displayed is from, columns with clashing names are qualified with the relation name. To avoid abiguity and problems in the future if new columns are added to relations, it is good practice to qualify column names in join conditions: SELECT * FROM $ satellites , $ planets WHERE $ satellites . planetId = $ planets . id ; Will return the same result as above, but be more resistant to future failure. Join queries of the kind seen thus far can also be written in this alternative form: SELECT * FROM $ satellites INNER JOIN $ planets ON $ satellites . planetId = $ planets . id ; The Opteryx planner currently uses a different execution strategy for these two similar queries, the explicit INNER JOIN style generally executes faster. Now we will figure out how we can get the Mercury and Venus records back in. What we want the query to do is to scan the $planets relation and for each row to find the matching $satellites row(s). If no matching row is found we want some \u201cempty values\u201d to be substituted for the $satellites relations columns. This kind of query is called an outer join. (The joins we have seen so far are inner joins and cross joins.) The command looks like this: SELECT * FROM $ satellites LEFT OUTER JOIN $ planets ON $ satellites . planetId = $ planets . id ; $satellites.id | planetId | $satellites.name | ... ---------------+----------+------------------+---- | 1 | | | 2 | | 1 | 3 | Moon | 2 | 4 | Phobos | 3 | 4 | Deimos | 4 | 5 | Io | 5 | 5 | Europa | (more rows and columns) Using the LEFT OUTER JOIN will mean the relation mentioned on the left of the join operator will have each of its rows in the output at least once, whereas the relation on the right will only have those rows output that match some row of the left relation. When outputting a left-relation row for which there is no right-relation match, empty (null) values are substituted for the right-relation columns. Note How null values are displayed may be different between different systems, common approaches are to display an empty cell or display 'none' or 'null' in an alternate format (e.g. italics or different font color). This is not controlled by Opteryx. Aggregate Functions Like most query engines and databases, Opteryx supports aggregate functions. An aggregate function computes a single result from multiple input rows. For example, there are aggregates to compute the COUNT , SUM , AVG (average), MAX (maximum) and MIN (minimum) over a set of rows.","title":"SQL Introduction"},{"location":"SQL%20Reference/01%20Introduction/#sql-introduction","text":"","title":"SQL Introduction"},{"location":"SQL%20Reference/01%20Introduction/#overview","text":"This page provides an overview of how to perform simple operations in SQL. This tutorial is only intended to give you an introduction and is not a complete tutorial on SQL. This tutorial is reworked from the DuckDB tutorial. All queries use the internal sample NASA datasets and should work regardless of the data your installation and set up has access to.","title":"Overview"},{"location":"SQL%20Reference/01%20Introduction/#concepts","text":"Opteryx is a system for querying ad hoc data stored in files as relations . A relation is mathematical term for a data table. Each relation is a named collection of rows, organized in columns, each column should be a common datatype. As an ad hoc query engine, the relations and their schema do not need to be predefined, they are determined at the time the query is run. This is one of the reasons Opteryx cannot be considered a RDBMS (relational database management system), even though it can be used to query data using SQL.","title":"Concepts"},{"location":"SQL%20Reference/01%20Introduction/#querying-relations","text":"To retrieve data from a relation, the relation is queried using a SQL SELECT statement. Basic statements are made of three parts; the list of columns to be returned and the list of relations to retrieve data from, and optional clauses to shape and filter the data that is returned. SELECT * FROM $ planets ; The * is shorthand for \"all columns\", by convention keywords are capitalized, and ; optionally terminates the query. SELECT id , name FROM $ planets WHERE name = 'Earth' ; The output of the above query should be id | name ----+------- 3 | Earth You can write functions, not just simple column references, in the select list. For example, you can write: SELECT id , UPPER ( name ) AS uppercase_name FROM $ planets WHERE id = 3 ; This should give: id | uppercase_name ----+---------------- 3 | EARTH Notice how the AS clause is used to relabel the output column. (The AS clause is optional.) A query can be \u201cqualified\u201d by adding a WHERE clause that specifies which rows are wanted. The WHERE clause contains a Boolean (truth value) expression, and only rows for which the Boolean expression is true are returned. The usual Boolean operators ( AND , OR , and NOT ) are allowed in the qualification. For example, the following the planets with fewer than 10 moons and a day longer than 24 hours: SELECT * FROM $ planets WHERE lengthOfDay > 24 AND numberOfMoons < 10 ; Result: name | lengthOfDay | numberOfMoons --------+-------------+--------------- Mercury | 4222.6 | 0 Venus | 2802 | 0 Mars | 24.7 | 2 Pluto | 153.3 | 5 The order of results are not guaranteed and should not be relied upon. If you request the results of the below query, you might get the Mercury or Venus in either order. Note The same query, of the same data in the same version of Opteryx will likely to return results in the same order, don't expect to test result order non-determinism by rerunning the query millions of times and looking for differences. These differences may manifest over different versions, or from subtle differences to the query statement or data. SELECT name , numberOfMoons FROM $ planets WHERE numberOfMoons = 0 ; Result: name | lengthOfDay | numberOfMoons --------+-------------+--------------- Mercury | 4222.6 | 0 Venus | 2802 | 0 But you\u2019d always get the results shown above if you do: SELECT name , numberOfMoons FROM $ planets WHERE numberOfMoons = 0 ORDER BY name ; You can request that duplicate rows be removed from the result of a query: SELECT DISTINCT planetId FROM $ satellites ; Result: planetId -------- 3 4 5 6 7 8 9 Here again, the result row ordering might vary. You can ensure consistent results by using DISTINCT and ORDER BY together: SELECT DISTINCT planetId FROM $ satellites ORDER BY planetId ;","title":"Querying Relations"},{"location":"SQL%20Reference/01%20Introduction/#joins-between-relations","text":"So far our queries have only accessed one relation at a time. Queries can access multiple relations at once, or access the same relation in such a way that multiple rows of the relation are being processed at the same time. A query that accesses multiple rows of the same or different relations at one time is called a join query. As an example, say you wish to list all the $satellites records together with the planet they orbit. To do that, we need to compare the planetId of each row of the $satellites relation with the id column of all rows in the $planets relation, and return the pairs of rows where these values match. This would be accomplished by the following query: SELECT * FROM $ satellites , $ planets WHERE planetId = $ planets . id ; $satellites.id | planetId | $satellites.name | ... ---------------+----------+------------------+---- 1 | 3 | Moon | 2 | 4 | Phobos | 3 | 4 | Deimos | 4 | 5 | Io | 5 | 5 | Europa | (more rows and columns) Observe two things about the result set: There are no result row for the planets of Mercury or Venus ( planetIds 1 and 2). This is because there is no matching entry in the $satellites relation for these planets, so the join ignores the unmatched rows in the $planets relation. Each of the relations being joined have an id and a name column, to ensure it is clear which relation the value being displayed is from, columns with clashing names are qualified with the relation name. To avoid abiguity and problems in the future if new columns are added to relations, it is good practice to qualify column names in join conditions: SELECT * FROM $ satellites , $ planets WHERE $ satellites . planetId = $ planets . id ; Will return the same result as above, but be more resistant to future failure. Join queries of the kind seen thus far can also be written in this alternative form: SELECT * FROM $ satellites INNER JOIN $ planets ON $ satellites . planetId = $ planets . id ; The Opteryx planner currently uses a different execution strategy for these two similar queries, the explicit INNER JOIN style generally executes faster. Now we will figure out how we can get the Mercury and Venus records back in. What we want the query to do is to scan the $planets relation and for each row to find the matching $satellites row(s). If no matching row is found we want some \u201cempty values\u201d to be substituted for the $satellites relations columns. This kind of query is called an outer join. (The joins we have seen so far are inner joins and cross joins.) The command looks like this: SELECT * FROM $ satellites LEFT OUTER JOIN $ planets ON $ satellites . planetId = $ planets . id ; $satellites.id | planetId | $satellites.name | ... ---------------+----------+------------------+---- | 1 | | | 2 | | 1 | 3 | Moon | 2 | 4 | Phobos | 3 | 4 | Deimos | 4 | 5 | Io | 5 | 5 | Europa | (more rows and columns) Using the LEFT OUTER JOIN will mean the relation mentioned on the left of the join operator will have each of its rows in the output at least once, whereas the relation on the right will only have those rows output that match some row of the left relation. When outputting a left-relation row for which there is no right-relation match, empty (null) values are substituted for the right-relation columns. Note How null values are displayed may be different between different systems, common approaches are to display an empty cell or display 'none' or 'null' in an alternate format (e.g. italics or different font color). This is not controlled by Opteryx.","title":"Joins Between Relations"},{"location":"SQL%20Reference/01%20Introduction/#aggregate-functions","text":"Like most query engines and databases, Opteryx supports aggregate functions. An aggregate function computes a single result from multiple input rows. For example, there are aggregates to compute the COUNT , SUM , AVG (average), MAX (maximum) and MIN (minimum) over a set of rows.","title":"Aggregate Functions"},{"location":"SQL%20Reference/02%20Statements/","text":"Statements Opteryx targets ANSI SQL compliant syntax. This standard alignment allows Opteryx users to quickly understand how to query data and enables easier porting of SQL between query engines and databases. EXPLAIN Show the logical execution plan of a statement. EXPLAIN statement The EXPLAIN clause outputs a summary of the execution plan for the query in the SELECT statement. Warning The data returned by the EXPLAIN statement is intended for interactive usage only and the output format may change between releases. Applications should not depend on the output of the EXPLAIN statement. SELECT Retrieve rows from zero or more relations. SELECT [ DISTINCT ] select_list FROM relation [ WITH ( NO_CACHE , NO_PARTITION )] [ INNER ] JOIN relation USING ( column ) CROSS JOIN relation LEFT [ OUTER ] JOIN relation RIGHT [ OUTER ] JOIN relation FULL [ OUTER ] JOIN relation ON condition FOR period WHERE condition GROUP BY groups HAVING group_filter ORDER BY order_expr OFFSET n LIMIT n SELECT clause SELECT [ DISTINCT ] expression [, ...] The SELECT clause specifies the list of columns that will be returned by the query. While it appears first in the clause, logically the expressions here are executed after most other clauses. The SELECT clause can contain arbitrary expressions that transform the output, as well as aggregate functions. The DISTINCT modifier is specified, only unique rows are included in the result set. In this case, each output column must be of a type that allows comparison. FROM / JOIN clauses FROM relation [, ...] [WITH (NOCACHE)] FROM relation [ INNER ] JOIN relation < USING (column) | ON condition > FROM relation < LEFT | RIGHT | FULL > [OUTER] JOIN relation FROM relation CROSS JOIN < relation | UNNEST(column) > The FROM clause specifies the source of the data on which the remainder of the query should operate. Logically, the FROM clause is where the query starts execution. The FROM clause can contain a single relation, a combination of multiple relations that are joined together, or another SELECT query inside a subquery node. JOIN clauses allow you to combine data from multiple relations. If no JOIN qualifier is provided, INNER will be used. JOIN qualifiers are mutually exclusive. ON and USING clauses are also mutually exclusive and can only be used with INNER and LEFT joins. See Joins for more information on JOIN syntax and functionality. Hints can be provided as part of the statement to direct the query planner and executor to make decisions. Relation hints are declared as WITH statements following a relation in the FROM and JOIN clauses, for example FROM $astronauts WITH (NOCACHE) . Reconised hints are: Hint Effect NOCACHE Ignores any cache configuration Note Hints are not guaranteed to be followed, the query planner and executor may ignore hints in specific circumstances. FOR clause FOR date FOR DATES BETWEEN start AND end FOR DATES IN range The FOR clause is a non ANSI SQL extension which filters data by the date it was recorded for. See Temporality for more information on FOR syntax and functionality. WHERE clause WHERE condition The WHERE clause specifies any filters to apply to the data. This allows you to select only a subset of the data in which you are interested. Logically the WHERE clause is applied immediately after the FROM clause. GROUP BY / HAVING clauses GROUP BY expression [, ...] HAVING group_filter The GROUP BY clause specifies which grouping columns should be used to perform any aggregations in the SELECT clause. If the GROUP BY clause is specified, the query is always an aggregate query, even if no aggregations are present in the SELECT clause. The HAVING clause specifies filters to apply to aggregated data, HAVING clauses require a GROUP BY clause. GROUP BY expressions may use column numbers, however, this is not recommended for statements intended for reuse. ORDER BY / LIMIT / OFFSET clauses ORDER BY expression [ ASC | DESC ] [, ...] OFFSET count LIMIT count ORDER BY , LIMIT and OFFSET are output modifiers. Logically they are applied at the very end of the query. The OFFSET clause discards initial rows from the returned set, the LIMIT clause restricts the amount of rows fetched, and the ORDER BY clause sorts the rows on the sorting criteria in either ascending or descending order. ORDER BY expressions may use column numbers, however, this is not recommended for statements intended for reuse. SHOW COLUMNS List the columns in a relation along with their data type and an indication if nulls have been found in the first page of records. SHOW [ EXTENDED ] [ FULL ] COLUMNS FROM relation LIKE pattern WHERE condition FOR period EXTENDED modifier Inclusion of the EXTENDED modifier includes summary statistics about the columns which take longer and more memory to create than the standard summary information without the modifier. The summary information varies between column types and values. FULL modifier Inclusion of the FULL modifier uses the entire dataset in order to return complete column information, rather than just the first page from the dataset. LIKE clause Specify a pattern in the optional LIKE clause to filter the results to the desired subset by the column name. WHERE clause The WHERE clause specifies any filters to apply to the data. This allows you to select only a subset of the data in which you are interested. Only one of LIKE and WHERE can be used in the same statement. FOR clause The FOR clause specifies the date to review data for. Although this supports the full syntax as per the SELECT statements, only one page of data is read in order to respond to SHOW COLUMNS statements. See Temporality for more information on FOR syntax and functionality.","title":"Statements"},{"location":"SQL%20Reference/02%20Statements/#statements","text":"Opteryx targets ANSI SQL compliant syntax. This standard alignment allows Opteryx users to quickly understand how to query data and enables easier porting of SQL between query engines and databases.","title":"Statements"},{"location":"SQL%20Reference/02%20Statements/#explain","text":"Show the logical execution plan of a statement. EXPLAIN statement The EXPLAIN clause outputs a summary of the execution plan for the query in the SELECT statement. Warning The data returned by the EXPLAIN statement is intended for interactive usage only and the output format may change between releases. Applications should not depend on the output of the EXPLAIN statement.","title":"EXPLAIN"},{"location":"SQL%20Reference/02%20Statements/#select","text":"Retrieve rows from zero or more relations. SELECT [ DISTINCT ] select_list FROM relation [ WITH ( NO_CACHE , NO_PARTITION )] [ INNER ] JOIN relation USING ( column ) CROSS JOIN relation LEFT [ OUTER ] JOIN relation RIGHT [ OUTER ] JOIN relation FULL [ OUTER ] JOIN relation ON condition FOR period WHERE condition GROUP BY groups HAVING group_filter ORDER BY order_expr OFFSET n LIMIT n","title":"SELECT"},{"location":"SQL%20Reference/02%20Statements/#select-clause","text":"SELECT [ DISTINCT ] expression [, ...] The SELECT clause specifies the list of columns that will be returned by the query. While it appears first in the clause, logically the expressions here are executed after most other clauses. The SELECT clause can contain arbitrary expressions that transform the output, as well as aggregate functions. The DISTINCT modifier is specified, only unique rows are included in the result set. In this case, each output column must be of a type that allows comparison.","title":"SELECT clause"},{"location":"SQL%20Reference/02%20Statements/#from-join-clauses","text":"FROM relation [, ...] [WITH (NOCACHE)] FROM relation [ INNER ] JOIN relation < USING (column) | ON condition > FROM relation < LEFT | RIGHT | FULL > [OUTER] JOIN relation FROM relation CROSS JOIN < relation | UNNEST(column) > The FROM clause specifies the source of the data on which the remainder of the query should operate. Logically, the FROM clause is where the query starts execution. The FROM clause can contain a single relation, a combination of multiple relations that are joined together, or another SELECT query inside a subquery node. JOIN clauses allow you to combine data from multiple relations. If no JOIN qualifier is provided, INNER will be used. JOIN qualifiers are mutually exclusive. ON and USING clauses are also mutually exclusive and can only be used with INNER and LEFT joins. See Joins for more information on JOIN syntax and functionality. Hints can be provided as part of the statement to direct the query planner and executor to make decisions. Relation hints are declared as WITH statements following a relation in the FROM and JOIN clauses, for example FROM $astronauts WITH (NOCACHE) . Reconised hints are: Hint Effect NOCACHE Ignores any cache configuration Note Hints are not guaranteed to be followed, the query planner and executor may ignore hints in specific circumstances.","title":"FROM / JOIN clauses"},{"location":"SQL%20Reference/02%20Statements/#for-clause","text":"FOR date FOR DATES BETWEEN start AND end FOR DATES IN range The FOR clause is a non ANSI SQL extension which filters data by the date it was recorded for. See Temporality for more information on FOR syntax and functionality.","title":"FOR clause"},{"location":"SQL%20Reference/02%20Statements/#where-clause","text":"WHERE condition The WHERE clause specifies any filters to apply to the data. This allows you to select only a subset of the data in which you are interested. Logically the WHERE clause is applied immediately after the FROM clause.","title":"WHERE clause"},{"location":"SQL%20Reference/02%20Statements/#group-by-having-clauses","text":"GROUP BY expression [, ...] HAVING group_filter The GROUP BY clause specifies which grouping columns should be used to perform any aggregations in the SELECT clause. If the GROUP BY clause is specified, the query is always an aggregate query, even if no aggregations are present in the SELECT clause. The HAVING clause specifies filters to apply to aggregated data, HAVING clauses require a GROUP BY clause. GROUP BY expressions may use column numbers, however, this is not recommended for statements intended for reuse.","title":"GROUP BY / HAVING clauses"},{"location":"SQL%20Reference/02%20Statements/#order-by-limit-offset-clauses","text":"ORDER BY expression [ ASC | DESC ] [, ...] OFFSET count LIMIT count ORDER BY , LIMIT and OFFSET are output modifiers. Logically they are applied at the very end of the query. The OFFSET clause discards initial rows from the returned set, the LIMIT clause restricts the amount of rows fetched, and the ORDER BY clause sorts the rows on the sorting criteria in either ascending or descending order. ORDER BY expressions may use column numbers, however, this is not recommended for statements intended for reuse.","title":"ORDER BY / LIMIT / OFFSET clauses"},{"location":"SQL%20Reference/02%20Statements/#show-columns","text":"List the columns in a relation along with their data type and an indication if nulls have been found in the first page of records. SHOW [ EXTENDED ] [ FULL ] COLUMNS FROM relation LIKE pattern WHERE condition FOR period","title":"SHOW COLUMNS"},{"location":"SQL%20Reference/02%20Statements/#extended-modifier","text":"Inclusion of the EXTENDED modifier includes summary statistics about the columns which take longer and more memory to create than the standard summary information without the modifier. The summary information varies between column types and values.","title":"EXTENDED modifier"},{"location":"SQL%20Reference/02%20Statements/#full-modifier","text":"Inclusion of the FULL modifier uses the entire dataset in order to return complete column information, rather than just the first page from the dataset.","title":"FULL modifier"},{"location":"SQL%20Reference/02%20Statements/#like-clause","text":"Specify a pattern in the optional LIKE clause to filter the results to the desired subset by the column name.","title":"LIKE clause"},{"location":"SQL%20Reference/02%20Statements/#where-clause_1","text":"The WHERE clause specifies any filters to apply to the data. This allows you to select only a subset of the data in which you are interested. Only one of LIKE and WHERE can be used in the same statement.","title":"WHERE clause"},{"location":"SQL%20Reference/02%20Statements/#for-clause_1","text":"The FOR clause specifies the date to review data for. Although this supports the full syntax as per the SELECT statements, only one page of data is read in order to respond to SHOW COLUMNS statements. See Temporality for more information on FOR syntax and functionality.","title":"FOR clause"},{"location":"SQL%20Reference/04%20Data%20Types/","text":"Data Types Opteryx uses a reduced set of types compared to full RDBMS platforms. Types Name Symbol Description BOOLEAN Logical boolean (True/False). NUMERIC All numeric types LIST An ordered sequence of data values. VARCHAR Variable-length character string. STRUCT A dictionary of multiple named values, where each key is a string, but the value can be a different type for each key. TIMESTAMP Combination of date and time. INTERVAL :fontawesome-solid-fa-arrows-left-right-to-line: The difference between two TIMESTAMP values OTHER None of the above or multiple types in the same column. Note INTERVAL has limited support OTHER is not a type, it is a catch-all when a type cannot be determined. Casting Values can be cast using the CAST function, its form is CAST(any AS type) . Coercion Timestamps Literal values in quotes may be in interpreted as a TIMESTAMP when they match a valid date in ISO 8601 format (e.g. YYYY-MM-DD and YYYY-MM-DD HH:MM ). All TIMESTAMP and date values read from datasets are coerced to nanosecond precision timestamps. Numbers All numeric values included in SQL statements and read from datasets are coerced to 64bit Floats.","title":"Data Types"},{"location":"SQL%20Reference/04%20Data%20Types/#data-types","text":"Opteryx uses a reduced set of types compared to full RDBMS platforms.","title":"Data Types"},{"location":"SQL%20Reference/04%20Data%20Types/#types","text":"Name Symbol Description BOOLEAN Logical boolean (True/False). NUMERIC All numeric types LIST An ordered sequence of data values. VARCHAR Variable-length character string. STRUCT A dictionary of multiple named values, where each key is a string, but the value can be a different type for each key. TIMESTAMP Combination of date and time. INTERVAL :fontawesome-solid-fa-arrows-left-right-to-line: The difference between two TIMESTAMP values OTHER None of the above or multiple types in the same column. Note INTERVAL has limited support OTHER is not a type, it is a catch-all when a type cannot be determined.","title":"Types"},{"location":"SQL%20Reference/04%20Data%20Types/#casting","text":"Values can be cast using the CAST function, its form is CAST(any AS type) .","title":"Casting"},{"location":"SQL%20Reference/04%20Data%20Types/#coercion","text":"","title":"Coercion"},{"location":"SQL%20Reference/04%20Data%20Types/#timestamps","text":"Literal values in quotes may be in interpreted as a TIMESTAMP when they match a valid date in ISO 8601 format (e.g. YYYY-MM-DD and YYYY-MM-DD HH:MM ). All TIMESTAMP and date values read from datasets are coerced to nanosecond precision timestamps.","title":"Timestamps"},{"location":"SQL%20Reference/04%20Data%20Types/#numbers","text":"All numeric values included in SQL statements and read from datasets are coerced to 64bit Floats.","title":"Numbers"},{"location":"SQL%20Reference/05%20Expressions/","text":"Expressions Logical The following logical operators are available: NOT , AND and OR . a b a AND b a OR b TRUE TRUE TRUE TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE The operators AND and OR are commutative, that is, you can switch the left and right operand without affecting the result. Comparison Operators Operator Description < less than > greater than <= less than or equal to >= greater than or equal to = equal to <> not equal to IN value in list NOT IN value not in list LIKE pattern match NOT LIKE inverse of LIKE ILIKE case-insensitive pattern match NOT ILIKE inverse of ILIKE ~ regular expression match !~ inverse of ~ Note When handling null and none values, infix inversions (e.g. x NOT LIKE y ) behave differently to prefix inversions ( NOT x LIKE y ). Other Comparisons Predicate Description a BETWEEN x AND y equivalent to a >= x AND a <= y a NOT BETWEEN x AND y equivalent to a < x OR a > y Using BETWEEN with other predicates, especially when used with an AND conjunction, can cause the query parser to fail. Sub Queries The IN operator can reference a sub query, this sub query cannot include a temporal clause ( FOR ), but otherwise the full syntax for SELECT queries are supported. For example, to find the planets without any satellites. SELECT name FROM $ planets WHERE id NOT IN ( SELECT DISTINCT planetId FROM $ satellites );","title":"Expressions"},{"location":"SQL%20Reference/05%20Expressions/#expressions","text":"","title":"Expressions"},{"location":"SQL%20Reference/05%20Expressions/#logical","text":"The following logical operators are available: NOT , AND and OR . a b a AND b a OR b TRUE TRUE TRUE TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE The operators AND and OR are commutative, that is, you can switch the left and right operand without affecting the result.","title":"Logical"},{"location":"SQL%20Reference/05%20Expressions/#comparison-operators","text":"Operator Description < less than > greater than <= less than or equal to >= greater than or equal to = equal to <> not equal to IN value in list NOT IN value not in list LIKE pattern match NOT LIKE inverse of LIKE ILIKE case-insensitive pattern match NOT ILIKE inverse of ILIKE ~ regular expression match !~ inverse of ~ Note When handling null and none values, infix inversions (e.g. x NOT LIKE y ) behave differently to prefix inversions ( NOT x LIKE y ).","title":"Comparison Operators"},{"location":"SQL%20Reference/05%20Expressions/#other-comparisons","text":"Predicate Description a BETWEEN x AND y equivalent to a >= x AND a <= y a NOT BETWEEN x AND y equivalent to a < x OR a > y Using BETWEEN with other predicates, especially when used with an AND conjunction, can cause the query parser to fail.","title":"Other Comparisons"},{"location":"SQL%20Reference/05%20Expressions/#sub-queries","text":"The IN operator can reference a sub query, this sub query cannot include a temporal clause ( FOR ), but otherwise the full syntax for SELECT queries are supported. For example, to find the planets without any satellites. SELECT name FROM $ planets WHERE id NOT IN ( SELECT DISTINCT planetId FROM $ satellites );","title":"Sub Queries"},{"location":"SQL%20Reference/06%20Functions/","text":"Functions Definitions noted with a \u266b accept different input arguments. Note Functions presently cannot be used with the outputs of function calls, for example DATEDIFF('year', birth_date, TODAY()) will return an error. List Functions For more details, see Working with Lists . array : list [ index : numeric ] \u2192 value \u266b \u2003Return the index th element from array . GET ( array : list , index : numeric ) \u2192 value \u266b \u2003Alias of array [ index ] . LEN ( array : list ) \u2192 numeric \u266b \u2003Alias of LENGTH ( array ). LENGTH ( array : list ) \u2192 numeric \u266b \u2003Returns the number of elements in array . LIST_CONTAINS ( array : list , value ) \u2192 boolean \u2003Return true if array contains value . See also SEARCH ( array , value ). LIST_CONTAINS_ANY ( array : list , values : list ) \u2192 boolean \u2003Return true if array contains any elements in values . LIST_CONTAINS_ALL ( array : list , values : list ) \u2192 boolean \u2003Return true if array contains all of elements in values . SEARCH ( array : list , value ) \u2192 boolean \u266b \u2003Return true if array contains value . Numeric Functions ABS ( x : numeric ) \u2192 numeric \u2003Alias of ABSOLUTE ( x ). ABSOLUTE ( x : numeric ) \u2192 numeric \u2003Returns the absolute value of x . CEIL ( x : numeric ) \u2192 numeric \u2003Alias of CEILING ( x ). CEILING ( x : numeric ) \u2192 numeric \u2003Returns x rounded up to the nearest integer. FLOOR ( x : numeric ) \u2192 numeric \u2003Returns x rounded down to the nearest integer. PI () \u2192 numeric \u2003Returns the constant Pi. ROUND ( x : numeric ) \u2192 numeric \u266b \u2003Returns x rounded to the nearest integer. ROUND ( x : numeric , places : numeric ) \u2192 numeric \u266b \u2003Returns x rounded to places decimal places. TRUNC ( x : numeric ) \u2192 numeric \u2003Alias of TRUNCATE ( x ). TRUNCATE ( x : numeric ) \u2192 numeric \u2003Returns x rounded to integer by dropping digits after decimal point. Text Functions Functions for examining and manipulating string values. str : varchar [ index : numeric ] \u2192 varchar \u266b \u2003Return the index th character from str . GET ( str : varchar , index : numeric ) \u2192 varchar \u266b \u2003Alias of str [ index ] . LEFT ( str : varchar , n : numeric ) \u2192 varchar \u2003Extract the left-most n characters of str . LEN ( str : varchar ) \u2192 numeric \u266b \u2003Alias of LENGTH ( str ) LENGTH ( str : varchar ) \u2192 numeric \u266b \u2003Returns the length of str in characters. LOWER ( str : varchar ) \u2192 varchar \u2003Converts str to lowercase. RIGHT ( str : varchar , n : numeric ) \u2192 varchar \u2003Extract the right-most n characters of str . SEARCH ( str : varchar , value : varchar ) \u2192 boolean \u266b \u2003Return True if str contains value . TRIM ( str : varchar ) \u2192 varchar \u2003Removes leading and trailing whitespace from str . UPPER ( str : varchar ) \u2192 varchar \u2003Converts str to uppercase. Date and Time Functions For more details, see Working with Timestamps . current_date \u2192 timestamp \u2003Return the current date, in UTC. Note current_date does not require parenthesis. current_time \u2192 timestamp \u2003Return the current date and time, in UTC. Note current_time does not require parenthesis. DATE ( ts : timestamp ) \u2192 timestamp \u2003Remove any time information, leaving just the date part of ts . DATE_FORMAT ( ts : timestamp , format : varchar ) \u2192 varchar \u2003Formats ts as a string using format . DATEPART ( unit : varchar , ts : timestamp ) \u2192 numeric \u2003Alias of EXTRACT ( unit FROM ts ). DATE_TRUNC ( unit : varchar , ts : timestamp ) \u2192 varchar \u2003Returns ts truncated to unit . DATEDIFF ( unit : varchar , start : timestamp , end : timestamp ) \u2192 numeric \u2003Calculate the difference between the start and end timestamps in a given unit . DAY ( timestamp ) \u2192 numeric \u2003Extract day number from a timestamp. See EXTRACT . EXTRACT ( unit FROM timestamp ) \u2192 numeric \u2003Extract unit of a timestamp. \u2003Also implemented as individual extraction functions. NOW () \u2192 timestamp \u2003Alias for current_time . TIME () \u2192 timestamp \u2003Current Time (UTC). TIME_BUCKET ( timestamp , multiple : numeric , unit : varchar ) \u2192 timestamp \u2003Floor timestamps into fixed time interval buckets. unit is optional and will be day if not provided. TODAY () \u2192 timestamp \u2003Alias for current_date . HOUR ( ts : timestamp ) \u2192 numeric \u2003Returns the hour of the day from ts . The value ranges from 0 to 23 . \u2003Alias for EXTRACT (hour FROM ts ). MINUTE ( ts : timestamp ) \u2192 numeric \u2003Returns the minute of the hour from ts . The value ranges from 0 to 59 . \u2003Alias for EXTRACT (minute FROM ts ) MONTH ( ts : timestamp ) \u2192 numeric \u2003Returns the month of the year from ts . The value ranges from 1 to 12 . \u2003Alias for EXTRACT (month FROM ts ) QUARTER ( ts : timestamp ) \u2192 numeric \u2003Returns the quarter of the year from ts . The value ranges from 1 to 4 . \u2003Alias for EXTRACT (quarter FROM ts ) SECOND ( ts : timestamp ) \u2192 numeric \u2003Returns the second of the minute from ts . The value ranges from 0 to 59 . \u2003Alias for EXTRACT (second FROM ts ) WEEK ( ts : timestamp ) \u2192 numeric \u2003Returns the week of the year from ts . The value ranges from 1 to 53 . \u2003Alias for EXTRACT (week FROM ts ) YEAR ( ts : timestamp ) \u2192 numeric \u2003Returns the year from ts . \u2003Alias for EXTRACT (year FROM ts ) Conversion Functions BOOLEAN ( any : any ) \u2192 boolean \u2003Cast any to a boolean . \u2003Alias for CAST ( any AS BOOLEAN). CAST ( any : any AS type ) \u2192 [type] \u2003Cast any to type . \u2003Also implemented as individual cast functions. NUMERIC ( any : any ) \u2192 numeric \u2003Cast any to a floating point number. \u2003Alias for CAST ( any AS NUMERIC). STRING ( any : any ) \u2192 varchar \u2003Alias of VARCHAR ( any ) and CAST ( any AS VARCHAR) TIMESTAMP ( iso8601 : varchar ) \u2192 timestamp \u266b \u2003Cast an ISO 8601 format string to a timestamp. \u2003Alias for CAST ( iso8601 AS TIMESTAMP). TIMESTAMP ( seconds : numeric ) \u2192 timestamp \u266b \u2003Return timestamp of seconds seconds since the Unix Epoch. TRY_CAST ( any : any AS type ) \u2192 [type] \u2003Cast any to type , failures return null . VARCHAR ( any ) \u2192 varchar \u2003Cast any to a string. \u2003Alias for CAST ( any AS VARCHAR). Struct Functions For more details, see Working with Structs . object : struct [ key : varchar ] \u2192 value \u266b \u2003Return the value for key from object . GET ( object : struct , key : varchar ) \u2192 value \u266b \u2003Alias of object [ key ] . SEARCH ( object : struct , value : varchar ) \u2192 boolean \u266b \u2003Return true if any of the values in object is value . System Functions VERSION () \u2192 varchar \u2003Return the version of Opteryx. Other Functions COALESCE ( arg1 , arg2 , ...) \u2192 [input type] \u2003Return the first item from args which is not null . GENERATE_SERIES ( stop : numeric ) \u2192 list < numeric > \u266b \u2003Return a numeric list between 1 and stop , with a step of 1. GENERATE_SERIES ( start : numeric , stop : numeric ) \u2192 list < numeric > \u266b \u2003Return a numeric list between start and stop , with a step of 1. GENERATE_SERIES ( start : numeric , stop : numeric , step : numeric ) \u2192 list < numeric > \u266b \u2003Return a numeric list between start and stop , with an increment of step . GENERATE_SERIES ( start : timestamp , stop : timestamp , interval ) \u2192 list < timestamp > \u266b \u2003Return a timestamp list between start and stop , with a interval of step . GENERATE_SERIES ( cidr : varchar ) \u2192 list < varchar > \u266b \u2003Return a list of IP addresses from a given cidr . HASH ( any ) \u2192 varchar \u2003Calculate the CityHash (64 bit). MD5 ( any ) \u2192 varchar \u2003Calculate the MD5 hash. RANDOM () \u2192 numeric \u2003Random number between 0.000 and 0.999. UNNEST ( array : list ) \u2192 relation \u2003Create a virtual relation with a row for each element in array .","title":"Functions"},{"location":"SQL%20Reference/06%20Functions/#functions","text":"Definitions noted with a \u266b accept different input arguments. Note Functions presently cannot be used with the outputs of function calls, for example DATEDIFF('year', birth_date, TODAY()) will return an error.","title":"Functions"},{"location":"SQL%20Reference/06%20Functions/#list-functions","text":"For more details, see Working with Lists . array : list [ index : numeric ] \u2192 value \u266b \u2003Return the index th element from array . GET ( array : list , index : numeric ) \u2192 value \u266b \u2003Alias of array [ index ] . LEN ( array : list ) \u2192 numeric \u266b \u2003Alias of LENGTH ( array ). LENGTH ( array : list ) \u2192 numeric \u266b \u2003Returns the number of elements in array . LIST_CONTAINS ( array : list , value ) \u2192 boolean \u2003Return true if array contains value . See also SEARCH ( array , value ). LIST_CONTAINS_ANY ( array : list , values : list ) \u2192 boolean \u2003Return true if array contains any elements in values . LIST_CONTAINS_ALL ( array : list , values : list ) \u2192 boolean \u2003Return true if array contains all of elements in values . SEARCH ( array : list , value ) \u2192 boolean \u266b \u2003Return true if array contains value .","title":"List Functions"},{"location":"SQL%20Reference/06%20Functions/#numeric-functions","text":"ABS ( x : numeric ) \u2192 numeric \u2003Alias of ABSOLUTE ( x ). ABSOLUTE ( x : numeric ) \u2192 numeric \u2003Returns the absolute value of x . CEIL ( x : numeric ) \u2192 numeric \u2003Alias of CEILING ( x ). CEILING ( x : numeric ) \u2192 numeric \u2003Returns x rounded up to the nearest integer. FLOOR ( x : numeric ) \u2192 numeric \u2003Returns x rounded down to the nearest integer. PI () \u2192 numeric \u2003Returns the constant Pi. ROUND ( x : numeric ) \u2192 numeric \u266b \u2003Returns x rounded to the nearest integer. ROUND ( x : numeric , places : numeric ) \u2192 numeric \u266b \u2003Returns x rounded to places decimal places. TRUNC ( x : numeric ) \u2192 numeric \u2003Alias of TRUNCATE ( x ). TRUNCATE ( x : numeric ) \u2192 numeric \u2003Returns x rounded to integer by dropping digits after decimal point.","title":"Numeric Functions"},{"location":"SQL%20Reference/06%20Functions/#text-functions","text":"Functions for examining and manipulating string values. str : varchar [ index : numeric ] \u2192 varchar \u266b \u2003Return the index th character from str . GET ( str : varchar , index : numeric ) \u2192 varchar \u266b \u2003Alias of str [ index ] . LEFT ( str : varchar , n : numeric ) \u2192 varchar \u2003Extract the left-most n characters of str . LEN ( str : varchar ) \u2192 numeric \u266b \u2003Alias of LENGTH ( str ) LENGTH ( str : varchar ) \u2192 numeric \u266b \u2003Returns the length of str in characters. LOWER ( str : varchar ) \u2192 varchar \u2003Converts str to lowercase. RIGHT ( str : varchar , n : numeric ) \u2192 varchar \u2003Extract the right-most n characters of str . SEARCH ( str : varchar , value : varchar ) \u2192 boolean \u266b \u2003Return True if str contains value . TRIM ( str : varchar ) \u2192 varchar \u2003Removes leading and trailing whitespace from str . UPPER ( str : varchar ) \u2192 varchar \u2003Converts str to uppercase.","title":"Text Functions"},{"location":"SQL%20Reference/06%20Functions/#date-and-time-functions","text":"For more details, see Working with Timestamps . current_date \u2192 timestamp \u2003Return the current date, in UTC. Note current_date does not require parenthesis. current_time \u2192 timestamp \u2003Return the current date and time, in UTC. Note current_time does not require parenthesis. DATE ( ts : timestamp ) \u2192 timestamp \u2003Remove any time information, leaving just the date part of ts . DATE_FORMAT ( ts : timestamp , format : varchar ) \u2192 varchar \u2003Formats ts as a string using format . DATEPART ( unit : varchar , ts : timestamp ) \u2192 numeric \u2003Alias of EXTRACT ( unit FROM ts ). DATE_TRUNC ( unit : varchar , ts : timestamp ) \u2192 varchar \u2003Returns ts truncated to unit . DATEDIFF ( unit : varchar , start : timestamp , end : timestamp ) \u2192 numeric \u2003Calculate the difference between the start and end timestamps in a given unit . DAY ( timestamp ) \u2192 numeric \u2003Extract day number from a timestamp. See EXTRACT . EXTRACT ( unit FROM timestamp ) \u2192 numeric \u2003Extract unit of a timestamp. \u2003Also implemented as individual extraction functions. NOW () \u2192 timestamp \u2003Alias for current_time . TIME () \u2192 timestamp \u2003Current Time (UTC). TIME_BUCKET ( timestamp , multiple : numeric , unit : varchar ) \u2192 timestamp \u2003Floor timestamps into fixed time interval buckets. unit is optional and will be day if not provided. TODAY () \u2192 timestamp \u2003Alias for current_date . HOUR ( ts : timestamp ) \u2192 numeric \u2003Returns the hour of the day from ts . The value ranges from 0 to 23 . \u2003Alias for EXTRACT (hour FROM ts ). MINUTE ( ts : timestamp ) \u2192 numeric \u2003Returns the minute of the hour from ts . The value ranges from 0 to 59 . \u2003Alias for EXTRACT (minute FROM ts ) MONTH ( ts : timestamp ) \u2192 numeric \u2003Returns the month of the year from ts . The value ranges from 1 to 12 . \u2003Alias for EXTRACT (month FROM ts ) QUARTER ( ts : timestamp ) \u2192 numeric \u2003Returns the quarter of the year from ts . The value ranges from 1 to 4 . \u2003Alias for EXTRACT (quarter FROM ts ) SECOND ( ts : timestamp ) \u2192 numeric \u2003Returns the second of the minute from ts . The value ranges from 0 to 59 . \u2003Alias for EXTRACT (second FROM ts ) WEEK ( ts : timestamp ) \u2192 numeric \u2003Returns the week of the year from ts . The value ranges from 1 to 53 . \u2003Alias for EXTRACT (week FROM ts ) YEAR ( ts : timestamp ) \u2192 numeric \u2003Returns the year from ts . \u2003Alias for EXTRACT (year FROM ts )","title":"Date and Time Functions"},{"location":"SQL%20Reference/06%20Functions/#conversion-functions","text":"BOOLEAN ( any : any ) \u2192 boolean \u2003Cast any to a boolean . \u2003Alias for CAST ( any AS BOOLEAN). CAST ( any : any AS type ) \u2192 [type] \u2003Cast any to type . \u2003Also implemented as individual cast functions. NUMERIC ( any : any ) \u2192 numeric \u2003Cast any to a floating point number. \u2003Alias for CAST ( any AS NUMERIC). STRING ( any : any ) \u2192 varchar \u2003Alias of VARCHAR ( any ) and CAST ( any AS VARCHAR) TIMESTAMP ( iso8601 : varchar ) \u2192 timestamp \u266b \u2003Cast an ISO 8601 format string to a timestamp. \u2003Alias for CAST ( iso8601 AS TIMESTAMP). TIMESTAMP ( seconds : numeric ) \u2192 timestamp \u266b \u2003Return timestamp of seconds seconds since the Unix Epoch. TRY_CAST ( any : any AS type ) \u2192 [type] \u2003Cast any to type , failures return null . VARCHAR ( any ) \u2192 varchar \u2003Cast any to a string. \u2003Alias for CAST ( any AS VARCHAR).","title":"Conversion Functions"},{"location":"SQL%20Reference/06%20Functions/#struct-functions","text":"For more details, see Working with Structs . object : struct [ key : varchar ] \u2192 value \u266b \u2003Return the value for key from object . GET ( object : struct , key : varchar ) \u2192 value \u266b \u2003Alias of object [ key ] . SEARCH ( object : struct , value : varchar ) \u2192 boolean \u266b \u2003Return true if any of the values in object is value .","title":"Struct Functions"},{"location":"SQL%20Reference/06%20Functions/#system-functions","text":"VERSION () \u2192 varchar \u2003Return the version of Opteryx.","title":"System Functions"},{"location":"SQL%20Reference/06%20Functions/#other-functions","text":"COALESCE ( arg1 , arg2 , ...) \u2192 [input type] \u2003Return the first item from args which is not null . GENERATE_SERIES ( stop : numeric ) \u2192 list < numeric > \u266b \u2003Return a numeric list between 1 and stop , with a step of 1. GENERATE_SERIES ( start : numeric , stop : numeric ) \u2192 list < numeric > \u266b \u2003Return a numeric list between start and stop , with a step of 1. GENERATE_SERIES ( start : numeric , stop : numeric , step : numeric ) \u2192 list < numeric > \u266b \u2003Return a numeric list between start and stop , with an increment of step . GENERATE_SERIES ( start : timestamp , stop : timestamp , interval ) \u2192 list < timestamp > \u266b \u2003Return a timestamp list between start and stop , with a interval of step . GENERATE_SERIES ( cidr : varchar ) \u2192 list < varchar > \u266b \u2003Return a list of IP addresses from a given cidr . HASH ( any ) \u2192 varchar \u2003Calculate the CityHash (64 bit). MD5 ( any ) \u2192 varchar \u2003Calculate the MD5 hash. RANDOM () \u2192 numeric \u2003Random number between 0.000 and 0.999. UNNEST ( array : list ) \u2192 relation \u2003Create a virtual relation with a row for each element in array .","title":"Other Functions"},{"location":"SQL%20Reference/07%20Aggregates/","text":"Aggregates Aggregates are functions that combine multiple rows into a single value. Aggregates can only be used in the SELECT and HAVING clauses of a SQL query. When the ORDER BY clause is provided, the values being aggregated are sorted after applying the function. Most aggregates require all of the data in the result set in order to complete, for large datasets this may result in memory issues; however, some aggregate functions have been written to run over huge datasets, COUNT , MIN , MAX , SUM . General Functions The table below shows the available general aggregate functions. (+) indicates aggregates optimized for large datasets. Unless noted, NONE values are ignored. Function Description LIST(a) Values in column 'a' returned as a list AVG(a) Average value for all values in column 'a', also AVERAGE COUNT(a) Number of values in column 'a' FIRST(a) First value in column 'a' (includes NULL ) LAST(a) Last value in column 'a', (includes NULL ) MAX(a) Maximum value in column 'a', also MAXIMUM MEDIAN(a) Middle value for values in column 'a' MIN(a) Minimum value in column 'a', also MINIMUM STDDEV_POP(a) Population standard deviation of values in column 'a' SUM(a) Cumulative sum value for all values in column 'a' VAR_POP(a) Population variance for values in column 'a'","title":"Aggregates"},{"location":"SQL%20Reference/07%20Aggregates/#aggregates","text":"Aggregates are functions that combine multiple rows into a single value. Aggregates can only be used in the SELECT and HAVING clauses of a SQL query. When the ORDER BY clause is provided, the values being aggregated are sorted after applying the function. Most aggregates require all of the data in the result set in order to complete, for large datasets this may result in memory issues; however, some aggregate functions have been written to run over huge datasets, COUNT , MIN , MAX , SUM .","title":"Aggregates"},{"location":"SQL%20Reference/07%20Aggregates/#general-functions","text":"The table below shows the available general aggregate functions. (+) indicates aggregates optimized for large datasets. Unless noted, NONE values are ignored. Function Description LIST(a) Values in column 'a' returned as a list AVG(a) Average value for all values in column 'a', also AVERAGE COUNT(a) Number of values in column 'a' FIRST(a) First value in column 'a' (includes NULL ) LAST(a) Last value in column 'a', (includes NULL ) MAX(a) Maximum value in column 'a', also MAXIMUM MEDIAN(a) Middle value for values in column 'a' MIN(a) Minimum value in column 'a', also MINIMUM STDDEV_POP(a) Population standard deviation of values in column 'a' SUM(a) Cumulative sum value for all values in column 'a' VAR_POP(a) Population variance for values in column 'a'","title":"General Functions"},{"location":"SQL%20Reference/08%20Joins/","text":"Joins Joins allow you to combine data from multiple relations into a single relation. CROSS JOIN FROM left_table CROSS JOIN right_table FROM left_table, right_table A CROSS JOIN returns the Cartesian product (all combinations) of two relations. Cross joins can either be specified using the explicit CROSS JOIN syntax or by specifying multiple relations in the FROM clause. SELECT * FROM left_table CROSS JOIN right_table ; The size of the resultant dataset when using CROSS JOIN is length of the two datasets multiplied together (2 x 3 = 6, in the pictorial example), which can easily result in extremely large datasets. When an alternate join approach is possible, it will almost always perform better than a CROSS JOIN . INNER JOIN FROM left_table INNER JOIN right_table [ ON condition | USING (column) ] FROM left_table JOIN right_table [ ON condition | USING (column) ] An INNER JOIN returns rows from both relations where the value in the joining column of one relation matches the value in the joining column of the other relation. Inner joins can either be specified using the full INNER JOIN syntax or the shorter JOIN syntax, and the joining column specified using ON condition or USING(column) syntax. SELECT * FROM left_table INNER JOIN right_table ON left_table . column_name = right_table . column_name ; In this example, the blue column is used as the joining column in both relations. Only the value 1 occurs in both relations so the resultant dataset is the combination of the row with 1 in right_table and the row with 1 in left_table . LEFT JOIN FROM left_table LEFT JOIN right_table ON condition FROM left_table LEFT OUTER JOIN right_table ON condition A LEFT JOIN returns all rows from the left relation, and rows from the right relation where there is a matching row, otherwise the fields for the right relation are populated with NULL . SELECT * FROM left_table LEFT OUTER JOIN right_table ON left_table . column_name = right_table . column_name ; RIGHT JOIN A RIGHT JOIN is the same as a LEFT JOIN with the relations swapped. FULL JOIN FROM left_table FULL JOIN right_table ON condition FROM left_table FULL OUTER JOIN right_table ON condition The FULL JOIN keyword returns all rows from the left relation, and all rows from the right relation. Where they have a matching value in the joining column, the rows will be aligned, otherwise the fields will be populated with NULL . SELECT * FROM left_table FULL OUTER JOIN right_table ON left_table . column_name = right_table . column_name ;","title":"Joins"},{"location":"SQL%20Reference/08%20Joins/#joins","text":"Joins allow you to combine data from multiple relations into a single relation.","title":"Joins"},{"location":"SQL%20Reference/08%20Joins/#cross-join","text":"FROM left_table CROSS JOIN right_table FROM left_table, right_table A CROSS JOIN returns the Cartesian product (all combinations) of two relations. Cross joins can either be specified using the explicit CROSS JOIN syntax or by specifying multiple relations in the FROM clause. SELECT * FROM left_table CROSS JOIN right_table ; The size of the resultant dataset when using CROSS JOIN is length of the two datasets multiplied together (2 x 3 = 6, in the pictorial example), which can easily result in extremely large datasets. When an alternate join approach is possible, it will almost always perform better than a CROSS JOIN .","title":"CROSS JOIN"},{"location":"SQL%20Reference/08%20Joins/#inner-join","text":"FROM left_table INNER JOIN right_table [ ON condition | USING (column) ] FROM left_table JOIN right_table [ ON condition | USING (column) ] An INNER JOIN returns rows from both relations where the value in the joining column of one relation matches the value in the joining column of the other relation. Inner joins can either be specified using the full INNER JOIN syntax or the shorter JOIN syntax, and the joining column specified using ON condition or USING(column) syntax. SELECT * FROM left_table INNER JOIN right_table ON left_table . column_name = right_table . column_name ; In this example, the blue column is used as the joining column in both relations. Only the value 1 occurs in both relations so the resultant dataset is the combination of the row with 1 in right_table and the row with 1 in left_table .","title":"INNER JOIN"},{"location":"SQL%20Reference/08%20Joins/#left-join","text":"FROM left_table LEFT JOIN right_table ON condition FROM left_table LEFT OUTER JOIN right_table ON condition A LEFT JOIN returns all rows from the left relation, and rows from the right relation where there is a matching row, otherwise the fields for the right relation are populated with NULL . SELECT * FROM left_table LEFT OUTER JOIN right_table ON left_table . column_name = right_table . column_name ;","title":"LEFT JOIN"},{"location":"SQL%20Reference/08%20Joins/#right-join","text":"A RIGHT JOIN is the same as a LEFT JOIN with the relations swapped.","title":"RIGHT JOIN"},{"location":"SQL%20Reference/08%20Joins/#full-join","text":"FROM left_table FULL JOIN right_table ON condition FROM left_table FULL OUTER JOIN right_table ON condition The FULL JOIN keyword returns all rows from the left relation, and all rows from the right relation. Where they have a matching value in the joining column, the rows will be aligned, otherwise the fields will be populated with NULL . SELECT * FROM left_table FULL OUTER JOIN right_table ON left_table . column_name = right_table . column_name ;","title":"FULL JOIN"},{"location":"SQL%20Reference/09%20Temporality/","text":"Temporality Temporality is the ability to view things as they were at a different point in time. Partition schemes that supports temporal queries allow you to view data from a different date by using a FOR clause in the SQL statement. FOR clauses state the date, or date range, a query should retrieve results for. Note If no temporal clause is provided and the schema supports it, FOR TODAY is assumed. Note Temporal clauses operate on calendar days. For example, from midnight FOR TODAY will return no data until data is written for that day. Single Dates Data from a specific, single, date can be obtained using the FOR date syntax. FOR date Date values in FOR clauses must either be in 'YYYY-MM-DD' format or a recognised date placeholder, for example. FOR TODAY FOR YESTERDAY FOR '2022-02-14' Date Ranges Data within a range of dates can be specified using FOR DATES BETWEEN or FOR DATES IN syntax. Where data is retrieved for multiple dates, the datasets for each day have an implicit UNION ALL applied to them. FOR DATES BETWEEN start AND end FOR DATES IN range Date values in BETWEEN clauses must either be in 'YYYY-MM-DD' format or a recognised date placeholder, for example: FOR DATES BETWEEN '2000-01-01' AND TODAY FOR DATES BETWEEN '2020-04-01' AND '2020-04-30' Date range values in IN clauses must be recognised date range placeholders, for example: FOR DATES IN LAST_MONTH Placeholders Placeholder Applicability Description TODAY FOR, BETWEEN This calendar day YESTERDAY FOR, BETWEEN The previous calendar day THIS_MONTH IN Since the first of the current month LAST_MONTH IN The previous calendar month (also PREVIOUS_MONTH ) Caution FOR clauses cannot contain comments or reference column values or aliases Dates can not include times and must be in the format 'YYYY-MM-DD' The default partition scheme does not support Temporal queries Only one temporal clause can be provided, the same dates will be used for all datasets in the query. If you are performing a JOIN or a subquery, the date or date ranges are applied to all datasets in the query.","title":"Temporality"},{"location":"SQL%20Reference/09%20Temporality/#temporality","text":"Temporality is the ability to view things as they were at a different point in time. Partition schemes that supports temporal queries allow you to view data from a different date by using a FOR clause in the SQL statement. FOR clauses state the date, or date range, a query should retrieve results for. Note If no temporal clause is provided and the schema supports it, FOR TODAY is assumed. Note Temporal clauses operate on calendar days. For example, from midnight FOR TODAY will return no data until data is written for that day.","title":"Temporality"},{"location":"SQL%20Reference/09%20Temporality/#single-dates","text":"Data from a specific, single, date can be obtained using the FOR date syntax. FOR date Date values in FOR clauses must either be in 'YYYY-MM-DD' format or a recognised date placeholder, for example. FOR TODAY FOR YESTERDAY FOR '2022-02-14'","title":"Single Dates"},{"location":"SQL%20Reference/09%20Temporality/#date-ranges","text":"Data within a range of dates can be specified using FOR DATES BETWEEN or FOR DATES IN syntax. Where data is retrieved for multiple dates, the datasets for each day have an implicit UNION ALL applied to them. FOR DATES BETWEEN start AND end FOR DATES IN range Date values in BETWEEN clauses must either be in 'YYYY-MM-DD' format or a recognised date placeholder, for example: FOR DATES BETWEEN '2000-01-01' AND TODAY FOR DATES BETWEEN '2020-04-01' AND '2020-04-30' Date range values in IN clauses must be recognised date range placeholders, for example: FOR DATES IN LAST_MONTH","title":"Date Ranges"},{"location":"SQL%20Reference/09%20Temporality/#placeholders","text":"Placeholder Applicability Description TODAY FOR, BETWEEN This calendar day YESTERDAY FOR, BETWEEN The previous calendar day THIS_MONTH IN Since the first of the current month LAST_MONTH IN The previous calendar month (also PREVIOUS_MONTH ) Caution FOR clauses cannot contain comments or reference column values or aliases Dates can not include times and must be in the format 'YYYY-MM-DD' The default partition scheme does not support Temporal queries Only one temporal clause can be provided, the same dates will be used for all datasets in the query. If you are performing a JOIN or a subquery, the date or date ranges are applied to all datasets in the query.","title":"Placeholders"},{"location":"SQL%20Reference/10%20Sample%20Data/","text":"Sample Data Opteryx has three built-in relations for demonstration and testing. $satellites (8 columns, 177 rows) $planets (20 columns, 9 rows) #plutoisaplanet $astronauts (19 columns, 357 rows) Satellite and Planet datasets where acquired from this source . Astronaut dataset acquired from Kaggle . These relations can be accessed as per user datasets, for example: SELECT * FROM $ planets ; Note A dataset called $no_table is used internally to represent no table has been specified, it has no know practical uses for users and may not exist in future releases.","title":"Sample Data"},{"location":"SQL%20Reference/10%20Sample%20Data/#sample-data","text":"Opteryx has three built-in relations for demonstration and testing. $satellites (8 columns, 177 rows) $planets (20 columns, 9 rows) #plutoisaplanet $astronauts (19 columns, 357 rows) Satellite and Planet datasets where acquired from this source . Astronaut dataset acquired from Kaggle . These relations can be accessed as per user datasets, for example: SELECT * FROM $ planets ; Note A dataset called $no_table is used internally to represent no table has been specified, it has no know practical uses for users and may not exist in future releases.","title":"Sample Data"},{"location":"SQL%20Reference/20%20Advanced/","text":"Advanced Hints are used to force the planner, optimizer or the executor to make specific decisions. If a hint is not recognized, it is ignored by the planner and executor, however is reported in the warnings. WITH hints FROM dataset WITH(NO_CACHE) Instructs blob/file reader to not use cache, regardless of other settings. FROM dataset WITH(NO_PARTITION) Instructs the blob/file reader to not use partitioning, regardless of other settings.","title":"Advanced"},{"location":"SQL%20Reference/20%20Advanced/#advanced","text":"Hints are used to force the planner, optimizer or the executor to make specific decisions. If a hint is not recognized, it is ignored by the planner and executor, however is reported in the warnings.","title":"Advanced"},{"location":"SQL%20Reference/20%20Advanced/#with-hints","text":"FROM dataset WITH(NO_CACHE) Instructs blob/file reader to not use cache, regardless of other settings. FROM dataset WITH(NO_PARTITION) Instructs the blob/file reader to not use partitioning, regardless of other settings.","title":"WITH hints"},{"location":"SQL%20Reference/Working%20with%20SQL/10%20Working%20with%20Timestamps/","text":"Working with Timestamps Actions Add/Subtract DATEDIFF(part, start, end) Extract EXTRACT(part FROM timestamp) DATE(timestamp) Format DATE_FORMAT(timestamp, format) Parse CAST(field AS TIMESTAMP) TIMESTAMP(field) Truncate DATE_TRUNC(part, timestamp) TIME_BUCKET(timestamp, multiple, unit) Generate current_date current_time YESTERDAY() TIME() generate_series() Note that current_date and current_time support being called without parenthesis. Recognized date parts and periods and support across various functions: Part DATE_TRUNC EXTRACT DATEDIFF TIME_BUCKET Notes second \u2713 \u2713 \u2713 \u2713 minute \u2713 \u2713 \u2713 \u2713 hour \u2713 \u2713 \u2713 \u2713 day \u2713 \u2713 \u2713 \u2713 dow \u2718 \u2713 \u2718 \u2718 day of week week \u2713 \u2713 \u2713 \u2713 iso week i.e. to monday month \u2713 \u2713 \u25b2 \u2713 DATEFIFF unreliable calculating months quarter \u2713 \u2713 \u2713 \u2713 doy \u2718 \u2713 \u2718 \u2718 day of year year \u2713 \u2713 \u2713 \u2713 Implicit Casting In many situation where a timestamp is expected, if an ISO1806 formatted string is provided, Opteryx will interpret as a timestamp. Timezones Opteryx is opinionated to run in UTC - all instances where the system time is requested, UTC is used.","title":"Working with Timestamps"},{"location":"SQL%20Reference/Working%20with%20SQL/10%20Working%20with%20Timestamps/#working-with-timestamps","text":"","title":"Working with Timestamps"},{"location":"SQL%20Reference/Working%20with%20SQL/10%20Working%20with%20Timestamps/#actions","text":"","title":"Actions"},{"location":"SQL%20Reference/Working%20with%20SQL/10%20Working%20with%20Timestamps/#addsubtract","text":"DATEDIFF(part, start, end)","title":"Add/Subtract"},{"location":"SQL%20Reference/Working%20with%20SQL/10%20Working%20with%20Timestamps/#extract","text":"EXTRACT(part FROM timestamp) DATE(timestamp)","title":"Extract"},{"location":"SQL%20Reference/Working%20with%20SQL/10%20Working%20with%20Timestamps/#format","text":"DATE_FORMAT(timestamp, format)","title":"Format"},{"location":"SQL%20Reference/Working%20with%20SQL/10%20Working%20with%20Timestamps/#parse","text":"CAST(field AS TIMESTAMP) TIMESTAMP(field)","title":"Parse"},{"location":"SQL%20Reference/Working%20with%20SQL/10%20Working%20with%20Timestamps/#truncate","text":"DATE_TRUNC(part, timestamp) TIME_BUCKET(timestamp, multiple, unit)","title":"Truncate"},{"location":"SQL%20Reference/Working%20with%20SQL/10%20Working%20with%20Timestamps/#generate","text":"current_date current_time YESTERDAY() TIME() generate_series() Note that current_date and current_time support being called without parenthesis. Recognized date parts and periods and support across various functions: Part DATE_TRUNC EXTRACT DATEDIFF TIME_BUCKET Notes second \u2713 \u2713 \u2713 \u2713 minute \u2713 \u2713 \u2713 \u2713 hour \u2713 \u2713 \u2713 \u2713 day \u2713 \u2713 \u2713 \u2713 dow \u2718 \u2713 \u2718 \u2718 day of week week \u2713 \u2713 \u2713 \u2713 iso week i.e. to monday month \u2713 \u2713 \u25b2 \u2713 DATEFIFF unreliable calculating months quarter \u2713 \u2713 \u2713 \u2713 doy \u2718 \u2713 \u2718 \u2718 day of year year \u2713 \u2713 \u2713 \u2713","title":"Generate"},{"location":"SQL%20Reference/Working%20with%20SQL/10%20Working%20with%20Timestamps/#implicit-casting","text":"In many situation where a timestamp is expected, if an ISO1806 formatted string is provided, Opteryx will interpret as a timestamp.","title":"Implicit Casting"},{"location":"SQL%20Reference/Working%20with%20SQL/10%20Working%20with%20Timestamps/#timezones","text":"Opteryx is opinionated to run in UTC - all instances where the system time is requested, UTC is used.","title":"Timezones"},{"location":"SQL%20Reference/Working%20with%20SQL/20%20Working%20with%20Lists/","text":"Working with Lists In Opteryx a list is an ordered collection of zero or more values of the same data type. Actions Accessing list[index] Testing value IN list Searching SEARCH(list, value) LIST_CONTAINS LIST_CONTAINS_ANY LIST_CONTAINS_ALL Converting Lists to Relations Using UNNEST UNNEST allows you to create a single column table either as a list of literals, or from a column of LIST type in a dataset. SELECT * FROM UNNEST (( True , False )) AS Booleans ;","title":"Working with Lists"},{"location":"SQL%20Reference/Working%20with%20SQL/20%20Working%20with%20Lists/#working-with-lists","text":"In Opteryx a list is an ordered collection of zero or more values of the same data type.","title":"Working with Lists"},{"location":"SQL%20Reference/Working%20with%20SQL/20%20Working%20with%20Lists/#actions","text":"","title":"Actions"},{"location":"SQL%20Reference/Working%20with%20SQL/20%20Working%20with%20Lists/#accessing","text":"list[index]","title":"Accessing"},{"location":"SQL%20Reference/Working%20with%20SQL/20%20Working%20with%20Lists/#testing","text":"value IN list","title":"Testing"},{"location":"SQL%20Reference/Working%20with%20SQL/20%20Working%20with%20Lists/#searching","text":"SEARCH(list, value) LIST_CONTAINS LIST_CONTAINS_ANY LIST_CONTAINS_ALL","title":"Searching"},{"location":"SQL%20Reference/Working%20with%20SQL/20%20Working%20with%20Lists/#converting-lists-to-relations","text":"","title":"Converting Lists to Relations"},{"location":"SQL%20Reference/Working%20with%20SQL/20%20Working%20with%20Lists/#using-unnest","text":"UNNEST allows you to create a single column table either as a list of literals, or from a column of LIST type in a dataset. SELECT * FROM UNNEST (( True , False )) AS Booleans ;","title":"Using UNNEST"},{"location":"SQL%20Reference/Working%20with%20SQL/30%20Working%20with%20Structs/","text":"Working with Structs In Opteryx a struct is a collection of zero or more key, value pairs. Keys must be VARCHAR , values can be different types. Actions Reading struct[key] Values within structs can be accessed by key using map notation, putting the key in square brackets following the struct. Example: SELECT birth_place [ 'town' ] FROM $ astronauts Searching `SEARCH(struct, value)` All values in a struct can be searched for a given value using the SEARCH function. Example: SELECT name , SEARCH ( birth_place , 'Italy' ) FROM $ astronauts","title":"Working with Structs"},{"location":"SQL%20Reference/Working%20with%20SQL/30%20Working%20with%20Structs/#working-with-structs","text":"In Opteryx a struct is a collection of zero or more key, value pairs. Keys must be VARCHAR , values can be different types.","title":"Working with Structs"},{"location":"SQL%20Reference/Working%20with%20SQL/30%20Working%20with%20Structs/#actions","text":"","title":"Actions"},{"location":"SQL%20Reference/Working%20with%20SQL/30%20Working%20with%20Structs/#reading","text":"struct[key] Values within structs can be accessed by key using map notation, putting the key in square brackets following the struct. Example: SELECT birth_place [ 'town' ] FROM $ astronauts","title":"Reading"},{"location":"SQL%20Reference/Working%20with%20SQL/30%20Working%20with%20Structs/#searching","text":"`SEARCH(struct, value)` All values in a struct can be searched for a given value using the SEARCH function. Example: SELECT name , SEARCH ( birth_place , 'Italy' ) FROM $ astronauts","title":"Searching"},{"location":"SQL%20Reference/Working%20with%20SQL/40%20Access%20Historical%20Data/","text":"Accessing Historical Data Opteryx lets you access Mabel partitioned data as at date or date range in the past. For datasets which are snapshots, this allows you to recall the data of that snapshop as at a data in the past. For datasets which are logs, this allows you to prune queries to just the dates which contain relevant data. Note Data must be Mabel partitioned or using a custom partition schema which supports data partitioning. Data returned for previous days with be the latest data as at today. For example if a backfill updates data from seven days ago, when querying that data today the backfilled data will be returned. There is no implicit deduplication of records as they are returned. Time Travel You can query dates or date ranges using a FOR clause in your query. For example to view the contents of partition SELECT * FROM $ planets FOR YESTERDAY ; This technique is well suited to viewing snapshotted datasets from a previoud point in time. Uranus was discovered in 1846 and Pluto was discovered in 1930, we could use the FOR clause to query the $planets relation from before that date like this SELECT name FROM $ planets FOR '1846-11-12' ; Which would return name ------- Mercury Venus Earth Mars Jupiter Saturn Neptune Note The above query is illustrative only demonstrate the concept and do not return these results when actually run. Accumulation For datasets which are continually added to, such as logs, the FOR clause can be used to quickly filter ranges of records to search over. The FOR clause will most likely record the date the record was written (the SYSTEM_TIME for the record) which may not be the same as the logical or effective date for a record, especially in situations where there is a lag in the records being recorded. The BETWEEN keyword can be used to describe ranges of records, SELECT name FROM $ planets FOR DATES BETWEEN '2021-01-01' and '2022-12-31' ; Note The above query is illustrative only demonstrate the concept and do not return the described results when actually run.","title":"Accessing Historical Data"},{"location":"SQL%20Reference/Working%20with%20SQL/40%20Access%20Historical%20Data/#accessing-historical-data","text":"Opteryx lets you access Mabel partitioned data as at date or date range in the past. For datasets which are snapshots, this allows you to recall the data of that snapshop as at a data in the past. For datasets which are logs, this allows you to prune queries to just the dates which contain relevant data. Note Data must be Mabel partitioned or using a custom partition schema which supports data partitioning. Data returned for previous days with be the latest data as at today. For example if a backfill updates data from seven days ago, when querying that data today the backfilled data will be returned. There is no implicit deduplication of records as they are returned.","title":"Accessing Historical Data"},{"location":"SQL%20Reference/Working%20with%20SQL/40%20Access%20Historical%20Data/#time-travel","text":"You can query dates or date ranges using a FOR clause in your query. For example to view the contents of partition SELECT * FROM $ planets FOR YESTERDAY ; This technique is well suited to viewing snapshotted datasets from a previoud point in time. Uranus was discovered in 1846 and Pluto was discovered in 1930, we could use the FOR clause to query the $planets relation from before that date like this SELECT name FROM $ planets FOR '1846-11-12' ; Which would return name ------- Mercury Venus Earth Mars Jupiter Saturn Neptune Note The above query is illustrative only demonstrate the concept and do not return these results when actually run.","title":"Time Travel"},{"location":"SQL%20Reference/Working%20with%20SQL/40%20Access%20Historical%20Data/#accumulation","text":"For datasets which are continually added to, such as logs, the FOR clause can be used to quickly filter ranges of records to search over. The FOR clause will most likely record the date the record was written (the SYSTEM_TIME for the record) which may not be the same as the logical or effective date for a record, especially in situations where there is a lag in the records being recorded. The BETWEEN keyword can be used to describe ranges of records, SELECT name FROM $ planets FOR DATES BETWEEN '2021-01-01' and '2022-12-31' ; Note The above query is illustrative only demonstrate the concept and do not return the described results when actually run.","title":"Accumulation"},{"location":"SQL%20Reference/Working%20with%20SQL/50%20Relation%20Constructors/","text":"Relation Constructors Opteryx provides options to create temporary relations as part of query definitions. These relations exist only for the execution of the query that defines them. Using VALUES VALUES allows you to create a multicolumn temporary relation where the values in the relation are explicitly defined in the statement. A simple example is as follows: SELECT * FROM ( VALUES ( 'High' , 3 ), ( 'Medium' , 2 ), ( 'Low' , 1 ) ) AS ratings ( name , rating ); Result: name | rating --------+-------- High | 3 Medium | 2 Low | 1 Using UNNEST UNNEST allows you to create a single column temporary relation where the values in the relation are explicitly defined in the statement. A simple example is as follows: SELECT * FROM UNNEST (( 1 , 2 , 3 )); Result: unnest -------- 1 2 3 Note The values in the UNNEST function are in two sets of parenthesis. The function accepts a list of values, parenthesis is used to wrap parameters to functions and also used to define lists. Using generate_series generate_series allows you to create series by defining the bounds of the series, and optionally, an interval to step between values in the created series. generate_series supports the following variations: Form Types Description generate_series(stop) NUMERIC Generate a NUMERIC series between 1 and 'stop', with a step of 1 generate_series(start, stop) NUMERIC, NUMERIC Generate a NUMERIC series between 'start' and 'stop', with a step of 1 generate_series(start, stop, step) NUMERIC, NUMERIC, NUMERIC Generate a NUMERIC series between 'start' and 'stop', with an explicit step size generate_series(start, stop, interval) TIMESTAMP, TIMESTAMP, INTERVAL Generate a TIMESTAMP series between 'start' and 'stop', with a given interval generate_series(cidr) VARCHAR Generate set of IP addresses from a given CIDR (e.g. 192.168.0.0/24 ) Single Parameter Example (NUMERIC): SELECT * FROM generate_series ( 3 ) generate_series ---------------- 1 2 3 Single Parameter Example (VARCHAR): SELECT * FROM generate_series ( '192.168.1.0/30' ) generate_series ---------------- 192.168.1.0 192.168.1.1 192.168.1.2 192.168.1.3 Two parameter Example: SELECT * FROM generate_series ( 2 , 4 ) generate_series ---------------- 2 3 4 Three parameter NUMERIC Example: SELECT * FROM generate_series ( - 5 , 5 , 5 ) generate_series ---------------- -5 0 5 Three parameter TIMESTAMP example: SELECT * FROM generate_series ( '2020-01-01' , '2025-12-31' , '1y' ) generate_series ---------------- 2020-01-01 00:00 2021-01-01 00:00 2022-01-01 00:00 2023-01-01 00:00 2024-01-01 00:00 2025-01-01 00:00 Interval Definitions Intervals are defined quantifying one or more periods which make up the interval, supported periods and their notation are: Recognized interval parts for the GENERATE_SERIES function are: Period Symbol Aliases Years year / years y / yr / yrs Months month / months mo / mon / mons / mth / mths Weeks week / weeks w / wk / wks Days day / days d Hours hour / hours h / hr / hrs Minutes minute / minutes m / min / mins Seconds second / seconds s / sec / secs Where required, periods can be combined to define more complex intervals, for example 1h30m represents one hour and 30 minutes. Using FAKE FAKE creates a table of random integers from provided row and column counts. This functionality has limited application outside of creating datasets for testing. A simple example is as follows: SELECT * FROM FAKE ( 3 , 2 ); Result: column_0 \u2502 column_1 ------------\u253c------------ 32981 \u2502 50883 5037 \u2502 42087 51741 \u2502 49456","title":"Relation Constructors"},{"location":"SQL%20Reference/Working%20with%20SQL/50%20Relation%20Constructors/#relation-constructors","text":"Opteryx provides options to create temporary relations as part of query definitions. These relations exist only for the execution of the query that defines them.","title":"Relation Constructors"},{"location":"SQL%20Reference/Working%20with%20SQL/50%20Relation%20Constructors/#using-values","text":"VALUES allows you to create a multicolumn temporary relation where the values in the relation are explicitly defined in the statement. A simple example is as follows: SELECT * FROM ( VALUES ( 'High' , 3 ), ( 'Medium' , 2 ), ( 'Low' , 1 ) ) AS ratings ( name , rating ); Result: name | rating --------+-------- High | 3 Medium | 2 Low | 1","title":"Using VALUES"},{"location":"SQL%20Reference/Working%20with%20SQL/50%20Relation%20Constructors/#using-unnest","text":"UNNEST allows you to create a single column temporary relation where the values in the relation are explicitly defined in the statement. A simple example is as follows: SELECT * FROM UNNEST (( 1 , 2 , 3 )); Result: unnest -------- 1 2 3 Note The values in the UNNEST function are in two sets of parenthesis. The function accepts a list of values, parenthesis is used to wrap parameters to functions and also used to define lists.","title":"Using UNNEST"},{"location":"SQL%20Reference/Working%20with%20SQL/50%20Relation%20Constructors/#using-generate_series","text":"generate_series allows you to create series by defining the bounds of the series, and optionally, an interval to step between values in the created series. generate_series supports the following variations: Form Types Description generate_series(stop) NUMERIC Generate a NUMERIC series between 1 and 'stop', with a step of 1 generate_series(start, stop) NUMERIC, NUMERIC Generate a NUMERIC series between 'start' and 'stop', with a step of 1 generate_series(start, stop, step) NUMERIC, NUMERIC, NUMERIC Generate a NUMERIC series between 'start' and 'stop', with an explicit step size generate_series(start, stop, interval) TIMESTAMP, TIMESTAMP, INTERVAL Generate a TIMESTAMP series between 'start' and 'stop', with a given interval generate_series(cidr) VARCHAR Generate set of IP addresses from a given CIDR (e.g. 192.168.0.0/24 ) Single Parameter Example (NUMERIC): SELECT * FROM generate_series ( 3 ) generate_series ---------------- 1 2 3 Single Parameter Example (VARCHAR): SELECT * FROM generate_series ( '192.168.1.0/30' ) generate_series ---------------- 192.168.1.0 192.168.1.1 192.168.1.2 192.168.1.3 Two parameter Example: SELECT * FROM generate_series ( 2 , 4 ) generate_series ---------------- 2 3 4 Three parameter NUMERIC Example: SELECT * FROM generate_series ( - 5 , 5 , 5 ) generate_series ---------------- -5 0 5 Three parameter TIMESTAMP example: SELECT * FROM generate_series ( '2020-01-01' , '2025-12-31' , '1y' ) generate_series ---------------- 2020-01-01 00:00 2021-01-01 00:00 2022-01-01 00:00 2023-01-01 00:00 2024-01-01 00:00 2025-01-01 00:00","title":"Using generate_series"},{"location":"SQL%20Reference/Working%20with%20SQL/50%20Relation%20Constructors/#interval-definitions","text":"Intervals are defined quantifying one or more periods which make up the interval, supported periods and their notation are: Recognized interval parts for the GENERATE_SERIES function are: Period Symbol Aliases Years year / years y / yr / yrs Months month / months mo / mon / mons / mth / mths Weeks week / weeks w / wk / wks Days day / days d Hours hour / hours h / hr / hrs Minutes minute / minutes m / min / mins Seconds second / seconds s / sec / secs Where required, periods can be combined to define more complex intervals, for example 1h30m represents one hour and 30 minutes.","title":"Interval Definitions"},{"location":"SQL%20Reference/Working%20with%20SQL/50%20Relation%20Constructors/#using-fake","text":"FAKE creates a table of random integers from provided row and column counts. This functionality has limited application outside of creating datasets for testing. A simple example is as follows: SELECT * FROM FAKE ( 3 , 2 ); Result: column_0 \u2502 column_1 ------------\u253c------------ 32981 \u2502 50883 5037 \u2502 42087 51741 \u2502 49456","title":"Using FAKE"},{"location":"SQL%20Reference/Working%20with%20SQL/60%20Sampling%20Data/","text":"SQL Tips Random Ordering of Results Order by RANDOM SELECT * FROM $ planets ORDER BY RANDOM ();","title":"SQL Tips"},{"location":"SQL%20Reference/Working%20with%20SQL/60%20Sampling%20Data/#sql-tips","text":"","title":"SQL Tips"},{"location":"SQL%20Reference/Working%20with%20SQL/60%20Sampling%20Data/#random-ordering-of-results","text":"Order by RANDOM SELECT * FROM $ planets ORDER BY RANDOM ();","title":"Random Ordering of Results"},{"location":"SQL%20Reference/Working%20with%20SQL/99%20Query%20Optimization/","text":"Query Optimization Adapted from 15 Best Practices for SQL Optimization . 1. Avoid using SELECT * Selecting only the fields you need to be returned improves query performance by reducing the amount of data that is processed internally. A principle the Query Optimizer uses is to eliminate rows and columns to process as early as possible, SELECT * removes the option to remove columns from the data being processed. 2. Prune Early Where available, use temporal filters ( FOR DATE ) to limit the date range over will limit the number of partitions that need need to be read. Not reading the record is faster than reading and working out if it needs to be filtered out of the result set. 3. GROUP BY field selection strings Grouping by string columns is slower than grouping by numeric columns, if you have an option of grouping by a username or a numeric user id, prefer the user id. cardinality Grouping by columns with high cardinality (mostly unique) is much slower than grouping where there is a lot of duplication in the groups. 4. Avoid CROSS JOIN Cross join will very likely create a lot of records that are not required - if you then filter these records from the two source tables using a WHERE clause, it's likely you should use an INNER JOIN instead. 5. Small table drives big table Most JOIN s require iterating over two tables, the left table, which is the one in the FROM clause, and the right table which is the one in the JOIN clause. It is generally faster to put the smaller table to the left . 6. Do not use too many values with the IN keyword 7. Replace subqueries with JOIN queries 8. Use the correct JOIN 9. Use LIMIT 10. Use WHERE to filter before GROUP BY 11. IS filters are generally faster than = IS NONE IS NOT NONE IS TRUE IS FALSE","title":"Query Optimization"},{"location":"SQL%20Reference/Working%20with%20SQL/99%20Query%20Optimization/#query-optimization","text":"Adapted from 15 Best Practices for SQL Optimization .","title":"Query Optimization"},{"location":"SQL%20Reference/Working%20with%20SQL/99%20Query%20Optimization/#1-avoid-using-select","text":"Selecting only the fields you need to be returned improves query performance by reducing the amount of data that is processed internally. A principle the Query Optimizer uses is to eliminate rows and columns to process as early as possible, SELECT * removes the option to remove columns from the data being processed.","title":"1. Avoid using SELECT *"},{"location":"SQL%20Reference/Working%20with%20SQL/99%20Query%20Optimization/#2-prune-early","text":"Where available, use temporal filters ( FOR DATE ) to limit the date range over will limit the number of partitions that need need to be read. Not reading the record is faster than reading and working out if it needs to be filtered out of the result set.","title":"2. Prune Early"},{"location":"SQL%20Reference/Working%20with%20SQL/99%20Query%20Optimization/#3-group-by-field-selection","text":"strings Grouping by string columns is slower than grouping by numeric columns, if you have an option of grouping by a username or a numeric user id, prefer the user id. cardinality Grouping by columns with high cardinality (mostly unique) is much slower than grouping where there is a lot of duplication in the groups.","title":"3. GROUP BY field selection"},{"location":"SQL%20Reference/Working%20with%20SQL/99%20Query%20Optimization/#4-avoid-cross-join","text":"Cross join will very likely create a lot of records that are not required - if you then filter these records from the two source tables using a WHERE clause, it's likely you should use an INNER JOIN instead.","title":"4. Avoid CROSS JOIN"},{"location":"SQL%20Reference/Working%20with%20SQL/99%20Query%20Optimization/#5-small-table-drives-big-table","text":"Most JOIN s require iterating over two tables, the left table, which is the one in the FROM clause, and the right table which is the one in the JOIN clause. It is generally faster to put the smaller table to the left .","title":"5. Small table drives big table"},{"location":"SQL%20Reference/Working%20with%20SQL/99%20Query%20Optimization/#6-do-not-use-too-many-values-with-the-in-keyword","text":"","title":"6. Do not use too many values with the IN keyword"},{"location":"SQL%20Reference/Working%20with%20SQL/99%20Query%20Optimization/#7-replace-subqueries-with-join-queries","text":"","title":"7. Replace subqueries with JOIN queries"},{"location":"SQL%20Reference/Working%20with%20SQL/99%20Query%20Optimization/#8-use-the-correct-join","text":"","title":"8. Use the correct JOIN"},{"location":"SQL%20Reference/Working%20with%20SQL/99%20Query%20Optimization/#9-use-limit","text":"","title":"9. Use LIMIT"},{"location":"SQL%20Reference/Working%20with%20SQL/99%20Query%20Optimization/#10-use-where-to-filter-before-group-by","text":"","title":"10. Use WHERE to filter before GROUP BY"},{"location":"SQL%20Reference/Working%20with%20SQL/99%20Query%20Optimization/#11-is-filters-are-generally-faster-than","text":"IS NONE IS NOT NONE IS TRUE IS FALSE","title":"11. IS filters are generally faster than ="}]}